
<!DOCTYPE html>


<html lang="en" data-content_root="./" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>Multiple Logistic Regression, Model Selection and Cross-validation &#8212; Data Science in Practice</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="_static/styles/theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="_static/styles/bootstrap.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="_static/styles/pydata-sphinx-theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />

  
  <link href="_static/vendor/fontawesome/6.5.2/css/all.min.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="_static/vendor/fontawesome/6.5.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="_static/vendor/fontawesome/6.5.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="_static/vendor/fontawesome/6.5.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="_static/pygments.css?v=03e43079" />
    <link rel="stylesheet" type="text/css" href="_static/styles/sphinx-book-theme.css?v=eba8b062" />
    <link rel="stylesheet" type="text/css" href="_static/togglebutton.css?v=9c3e77be" />
    <link rel="stylesheet" type="text/css" href="_static/copybutton.css?v=76b2166b" />
    <link rel="stylesheet" type="text/css" href="_static/mystnb.8ecb98da25f57f5357bf6f572d296f466b2cfe2517ffebfabe82451661e28f02.css" />
    <link rel="stylesheet" type="text/css" href="_static/sphinx-thebe.css?v=4fa983c6" />
    <link rel="stylesheet" type="text/css" href="_static/sphinx-design.min.css?v=95c83b7e" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b" />
<link rel="preload" as="script" href="_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b" />
  <script src="_static/vendor/fontawesome/6.5.2/js/all.min.js?digest=dfe6caa3a7d634c4db9b"></script>

    <script src="_static/documentation_options.js?v=9eb32ce0"></script>
    <script src="_static/doctools.js?v=9a2dae69"></script>
    <script src="_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="_static/clipboard.min.js?v=a7894cd8"></script>
    <script src="_static/copybutton.js?v=f281be69"></script>
    <script src="_static/scripts/sphinx-book-theme.js?v=887ef09a"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="_static/togglebutton.js?v=1ae7504c"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="_static/design-tabs.js?v=f930bc37"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script async="async" src="_static/sphinx-thebe.js?v=c100c467"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'logistic_regression_cross_validation';</script>
    <link rel="canonical" href="https://lisds.github.io/dsip/logistic_regression_cross_validation.html" />
    <link rel="icon" href="_static/dsfe_favicon.ico"/>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="Fitting models with different cost functions" href="cost_functions.html" />
    <link rel="prev" title="Statistical Adjustment in Multi-predictor Linear Regression Models" href="multiple_predictors_statistical_adjustment.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <div id="pst-skip-link" class="skip-link d-print-none"><a href="#main-content">Skip to main content</a></div>
  
  <div id="pst-scroll-pixel-helper"></div>
  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>Back to top</button>

  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-primary-sidebar-checkbox"/>
  <label class="overlay overlay-primary" for="pst-primary-sidebar-checkbox"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-secondary-sidebar-checkbox"/>
  <label class="overlay overlay-secondary" for="pst-secondary-sidebar-checkbox"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search this book..."
         aria-label="Search this book..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>

  <div class="pst-async-banner-revealer d-none">
  <aside id="bd-header-version-warning" class="d-none d-print-none" aria-label="Version warning"></aside>
</div>

  
    <header class="bd-header navbar navbar-expand-lg bd-navbar d-print-none">
    </header>
  

  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">

  
    
  

<a class="navbar-brand logo" href="intro.html">
  
  
  
  
  
    
    
      
    
    
    <img src="_static/dsfe_logo.png" class="logo__image only-light" alt="Data Science in Practice - Home"/>
    <script>document.write(`<img src="_static/dsfe_logo.png" class="logo__image only-dark" alt="Data Science in Practice - Home"/>`);</script>
  
  
</a></div>
        <div class="sidebar-primary-item">

 <script>
 document.write(`
   <button class="btn search-button-field search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass"></i>
    <span class="search-button__default-text">Search</span>
    <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
   </button>
 `);
 </script></div>
        <div class="sidebar-primary-item"><nav class="bd-links bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        <p aria-level="2" class="caption" role="heading"><span class="caption-text">Coding background</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="assert.html">Using <code class="docutils literal notranslate"><span class="pre">assert</span></code> for testing</a></li>
<li class="toctree-l1"><a class="reference internal" href="arrays_and_images.html">Arrays as images, images as arrays</a></li>
<li class="toctree-l1"><a class="reference internal" href="where_and_argmin.html">Where and argmin</a></li>
<li class="toctree-l1"><a class="reference internal" href="where_2d.html">Where in 2D</a></li>
<li class="toctree-l1"><a class="reference internal" href="image_for_optimize.html">2D arrays, images, optimization</a></li>
<li class="toctree-l1"><a class="reference internal" href="pathlib.html">Using the <code class="docutils literal notranslate"><span class="pre">pathlib</span></code> module</a></li>
<li class="toctree-l1"><a class="reference internal" href="dictionaries.html">Dictionaries</a></li>
<li class="toctree-l1"><a class="reference internal" href="pandas_and_dicts.html">Pandas and dictionaries</a></li>
<li class="toctree-l1"><a class="reference internal" href="lambda_functions.html">Lambda functions</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Reading formulae</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="on_vectors.html">Vectors and dot products</a></li>
<li class="toctree-l1"><a class="reference internal" href="numpy_and_vectors.html">Numpy, arrays, and vectors</a></li>
<li class="toctree-l1"><a class="reference internal" href="algebra_of_sums.html">Some algebra with summation</a></li>
<li class="toctree-l1"><a class="reference internal" href="lin_regression_notation.html">Notation for linear regression models</a></li>
<li class="toctree-l1"><a class="reference internal" href="vector_projection.html">Vector projection</a></li>
<li class="toctree-l1"><a class="reference internal" href="pca_introduction.html">Introducing principal component analysis</a></li>
<li class="toctree-l1"><a class="reference internal" href="pca_dimension_reduction.html">Dimension reduction with Principal Component Analysis</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Intermediate regression</span></p>
<ul class="current nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="lin_regression_multiple_predictors.html">Linear regression notation for models with multiple predictors</a></li>
<li class="toctree-l1"><a class="reference internal" href="epl_modeling.html">Attack or defense?  An example of multiple regression</a></li>
<li class="toctree-l1"><a class="reference internal" href="linear_regression_categorical_predictors.html">Categorical predictors in linear regression</a></li>
<li class="toctree-l1"><a class="reference internal" href="multiple_predictors_statistical_adjustment.html">Statistical Adjustment in Multi-predictor Linear Regression Models</a></li>
<li class="toctree-l1 current active"><a class="current reference internal" href="#">Multiple Logistic Regression, Model Selection and Cross-validation</a></li>
<li class="toctree-l1"><a class="reference internal" href="cost_functions.html">Fitting models with different cost functions</a></li>
<li class="toctree-l1"><a class="reference internal" href="matrix_notation.html">Expressing the linear model with matrices</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Classification</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="naive_bayes.html">Naive Bayes classifiers</a></li>
<li class="toctree-l1"><a class="reference internal" href="k_means.html">k-means clustering</a></li>
<li class="toctree-l1"><a class="reference internal" href="kernel_density_estimation.html">Kernel density estimation</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Working with data</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="data_work.html">The problem with data</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">The tools</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="installation.html">Installing on your computer</a></li>
<li class="toctree-l1"><a class="reference internal" href="the-problem-with-notebooks.html">The joys and sorrows of notebooks</a></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main" role="main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article d-print-none">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><button class="sidebar-toggle primary-toggle btn btn-sm" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</button></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">





<div class="dropdown dropdown-launch-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Launch interactive content">
    <i class="fas fa-rocket"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="https://mybinder.org/v2/gh/lisds/dsip/main?urlpath=lab/tree/logistic_regression_cross_validation.Rmd" target="_blank"
   class="btn btn-sm dropdown-item"
   title="Launch on Binder"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  
    <img alt="Binder logo" src="_static/images/logo_binder.svg">
  </span>
<span class="btn__text-container">Binder</span>
</a>
</li>
      
      
      
      
      <li><a href="https://ds.lis.2i2c.cloud/hub/user-redirect/git-pull?repo=https%3A//github.com/lisds/dsip&urlpath=lab/tree/dsip/logistic_regression_cross_validation.Rmd&branch=main" target="_blank"
   class="btn btn-sm dropdown-item"
   title="Launch on JupyterHub"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  
    <img alt="JupyterHub logo" src="_static/images/logo_jupyterhub.svg">
  </span>
<span class="btn__text-container">JupyterHub</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="initThebeSBT()"
  class="btn btn-sm btn-launch-thebe dropdown-item"
  title="Launch Thebe"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-play"></i>
  </span>
<span class="btn__text-container">Live Code</span>
</button>
</li>
      
  </ul>
</div>






<div class="dropdown dropdown-source-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Source repositories">
    <i class="fab fa-github"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="https://github.com/lisds/dsip" target="_blank"
   class="btn btn-sm btn-source-repository-button dropdown-item"
   title="Source repository"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fab fa-github"></i>
  </span>
<span class="btn__text-container">Repository</span>
</a>
</li>
      
      
      
      
      <li><a href="https://github.com/lisds/dsip/edit/main/logistic_regression_cross_validation.Rmd" target="_blank"
   class="btn btn-sm btn-source-edit-button dropdown-item"
   title="Suggest edit"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-pencil-alt"></i>
  </span>
<span class="btn__text-container">Suggest edit</span>
</a>
</li>
      
      
      
      
      <li><a href="https://github.com/lisds/dsip/issues/new?title=Issue%20on%20page%20%2Flogistic_regression_cross_validation.html&body=Your%20issue%20content%20here." target="_blank"
   class="btn btn-sm btn-source-issues-button dropdown-item"
   title="Open an issue"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-lightbulb"></i>
  </span>
<span class="btn__text-container">Open issue</span>
</a>
</li>
      
  </ul>
</div>






<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="_sources/logistic_regression_cross_validation.Rmd" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.Rmd</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>



<script>
document.write(`
  <button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="theme-switch fa-solid fa-sun fa-lg" data-mode="light"></i>
    <i class="theme-switch fa-solid fa-moon fa-lg" data-mode="dark"></i>
    <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"></i>
  </button>
`);
</script>


<script>
document.write(`
  <button class="btn btn-sm pst-navbar-icon search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass fa-lg"></i>
  </button>
`);
</script>
<button class="sidebar-toggle secondary-toggle btn btn-sm" title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</button>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>Multiple Logistic Regression, Model Selection and Cross-validation</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#more-logistic-regression">More Logistic Regression</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#our-first-logistic-model-and-two-perspectives-on-the-cost-function">Our first logistic model - and two perspectives on the cost function</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#logistic-regression-in-3d-i-e-with-two-predictors">Logistic regression in 3D (i.e. with two predictors)</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#model-evaluation-goodness-of-fit-and-model-comparison">Model Evaluation (Goodness-of-Fit) and Model Comparison</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#accuracy-for-models-with-a-categorical-outcome-variable">Accuracy (for models with a categorical outcome variable)</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#test-train-splits">Test/Train splits</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#cross-validation">Cross-validation</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#other-ways-of-model-building">Other ways of model-building</a></li>
</ul>
            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article">
                  
  <section class="tex2jax_ignore mathjax_ignore" id="multiple-logistic-regression-model-selection-and-cross-validation">
<h1>Multiple Logistic Regression, Model Selection and Cross-validation<a class="headerlink" href="#multiple-logistic-regression-model-selection-and-cross-validation" title="Link to this heading">#</a></h1>
<p>On this page we will cover:</p>
<ul class="simple">
<li><p>a bit more on the logistic regression cost function</p></li>
<li><p>logistic regression models with multiple predictors</p></li>
<li><p>model selection/building using cross-validation and grid search</p></li>
<li><p>model selection using/building Akaike Information Criterion (if there is time)</p></li>
</ul>
<p>In data science, typically we have a sample of observational units, and we are
interested in the underlying population from which those observational units
were drawn.</p>
<p>Model selection involves the process of evaluating which model we should use to
best describe and predict using the data we have, whilst attempting to ensure
that our model parameters will generalize to unseen data from the same population.</p>
<p>This can involve choosing which <em>type</em> of model to use (e.g. which regression model or
machine learning technique) as well as choosing which predictors should be in our model.</p>
<p>The model selection components of this class apply to other regression models (like linear regression) and
to other machine learning techniques also, not just to logistic regression.</p>
<p>On this page we will do the following:</p>
<ul class="simple">
<li><p>fit a single predictor logistic regression, and go into a bit more detail about the cost function</p></li>
<li><p>fit a two predictor logistic regression, to see how it works with more than one predictor</p></li>
<li><p>look at model evaluation and selection techniques to decided which model is better (e.g. do we need the second predictor)?</p></li>
</ul>
<p>First, let’s import some libraries/functions to set up the page:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">pandas</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">pd</span>
<span class="c1"># Enable copy_on_write for Pandas&lt;3</span>
<span class="k">if</span> <span class="nb">int</span><span class="p">(</span><span class="n">pd</span><span class="o">.</span><span class="n">__version__</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s1">&#39;.&#39;</span><span class="p">)[</span><span class="mi">0</span><span class="p">])</span> <span class="o">&lt;</span> <span class="mi">3</span><span class="p">:</span>
    <span class="n">pd</span><span class="o">.</span><span class="n">set_option</span><span class="p">(</span><span class="s1">&#39;mode.copy_on_write&#39;</span><span class="p">,</span> <span class="kc">True</span><span class="p">)</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">matplotlib.pyplot</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">plt</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">plotly.express</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">px</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">plotly.graph_objects</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">go</span>
<span class="c1"># The formula interface for Statsmodels</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">statsmodels.formula.api</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">smf</span>
<span class="c1"># Some printing functions</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">jupyprint</span><span class="w"> </span><span class="kn">import</span> <span class="n">arraytex</span><span class="p">,</span> <span class="n">jupyprint</span>
<span class="c1"># Optimization function</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">scipy.optimize</span><span class="w"> </span><span class="kn">import</span> <span class="n">minimize</span>
<span class="c1"># For interactive widgets.</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">ipywidgets</span><span class="w"> </span><span class="kn">import</span> <span class="n">interact</span>

<span class="c1"># interactive plotting function</span>
<span class="k">def</span><span class="w"> </span><span class="nf">plotly_3D_with_plane</span><span class="p">(</span><span class="n">dataset</span><span class="p">,</span> <span class="n">x1_string</span><span class="p">,</span> <span class="n">x2_string</span><span class="p">,</span> <span class="n">y_string</span><span class="p">,</span> <span class="n">hover_string_list</span><span class="p">,</span>
                         <span class="n">x1_slope</span><span class="p">,</span> <span class="n">x2_slope</span><span class="p">,</span> <span class="n">intercept</span><span class="p">,</span> <span class="n">model_title_string</span><span class="o">=</span><span class="s1">&#39;&#39;</span><span class="p">,</span>
                        <span class="n">y_1_or_0</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                        <span class="n">probability</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Interactive 3D scatter, via plotly.&quot;&quot;&quot;</span>
    
    <span class="c1"># create the scatterplot</span>
    <span class="n">scatter_3d</span> <span class="o">=</span> <span class="n">px</span><span class="o">.</span><span class="n">scatter_3d</span><span class="p">(</span><span class="n">dataset</span><span class="p">,</span> <span class="n">x</span><span class="o">=</span><span class="n">x1_string</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="n">x2_string</span><span class="p">,</span> <span class="n">z</span><span class="o">=</span><span class="n">y_string</span><span class="p">,</span>
                          <span class="n">hover_data</span><span class="o">=</span> <span class="n">hover_string_list</span><span class="p">,</span> <span class="n">symbol</span><span class="o">=</span><span class="s1">&#39;survived&#39;</span><span class="p">,</span>
                               <span class="n">color</span><span class="o">=</span><span class="s1">&#39;survived&#39;</span><span class="p">,</span>
                              <span class="n">symbol_map</span><span class="o">=</span><span class="p">{</span><span class="mi">1</span><span class="p">:</span><span class="s1">&#39;x&#39;</span><span class="p">,</span> <span class="mi">0</span><span class="p">:</span><span class="s1">&#39;o&#39;</span><span class="p">})</span>

    <span class="c1"># generate the regression surface</span>
    <span class="n">x1</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">min</span><span class="p">(</span><span class="n">dataset</span><span class="p">[</span><span class="n">x1_string</span><span class="p">]),</span> <span class="n">np</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">dataset</span><span class="p">[</span><span class="n">x1_string</span><span class="p">]))</span>
    <span class="n">x2</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">min</span><span class="p">(</span><span class="n">dataset</span><span class="p">[</span><span class="n">x2_string</span><span class="p">]),</span> <span class="n">np</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">dataset</span><span class="p">[</span><span class="n">x2_string</span><span class="p">]))</span>
    <span class="n">x1</span><span class="p">,</span> <span class="n">x2</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">meshgrid</span><span class="p">(</span><span class="n">x1</span><span class="p">,</span> <span class="n">x2</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">probability</span> <span class="o">==</span> <span class="kc">False</span><span class="p">:</span>
        <span class="n">y</span> <span class="o">=</span> <span class="n">x1_slope</span> <span class="o">*</span> <span class="n">x1</span> <span class="o">+</span> <span class="n">x2_slope</span> <span class="o">*</span> <span class="n">x2</span> <span class="o">+</span> <span class="n">intercept</span>
    <span class="k">elif</span> <span class="n">probability</span> <span class="o">==</span> <span class="kc">True</span><span class="p">:</span>
        <span class="n">y</span> <span class="o">=</span> <span class="n">inverse_logit</span><span class="p">(</span><span class="n">x1_slope</span> <span class="o">*</span> <span class="n">x1</span> <span class="o">+</span> <span class="n">x2_slope</span> <span class="o">*</span> <span class="n">x2</span> <span class="o">+</span> <span class="n">intercept</span><span class="p">)</span>
    
    <span class="n">scatter_3d</span><span class="o">.</span><span class="n">add_trace</span><span class="p">(</span><span class="n">go</span><span class="o">.</span><span class="n">Surface</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="n">x1</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="n">x2</span><span class="p">,</span> <span class="n">z</span><span class="o">=</span><span class="n">y</span><span class="p">,</span> <span class="n">opacity</span><span class="o">=</span><span class="mf">0.6</span><span class="p">))</span>
                     
    <span class="c1"># add a title to the plot and adjust view angle</span>
    <span class="n">scatter_3d</span><span class="o">.</span><span class="n">update_layout</span><span class="p">(</span><span class="n">title</span><span class="o">=</span><span class="n">model_title_string</span><span class="p">,</span>
                             <span class="n">scene</span><span class="o">=</span><span class="p">{</span><span class="s1">&#39;camera&#39;</span><span class="p">:</span> <span class="p">{</span><span class="s1">&#39;up&#39;</span><span class="p">:</span> <span class="p">{</span><span class="s1">&#39;x&#39;</span><span class="p">:</span> <span class="mi">0</span><span class="p">,</span> <span class="s1">&#39;y&#39;</span><span class="p">:</span> <span class="mi">0</span><span class="p">,</span> <span class="s1">&#39;z&#39;</span><span class="p">:</span> <span class="mi">1</span><span class="p">},</span>
                                    <span class="s1">&#39;center&#39;</span><span class="p">:</span> <span class="p">{</span><span class="s1">&#39;x&#39;</span><span class="p">:</span> <span class="mi">0</span><span class="p">,</span> <span class="s1">&#39;y&#39;</span><span class="p">:</span> <span class="mi">0</span><span class="p">,</span> <span class="s1">&#39;z&#39;</span><span class="p">:</span> <span class="mi">0</span><span class="p">},</span>
                                    <span class="s1">&#39;eye&#39;</span><span class="p">:</span> <span class="p">{</span><span class="s1">&#39;x&#39;</span><span class="p">:</span> <span class="mf">1.6</span><span class="p">,</span> <span class="s1">&#39;y&#39;</span><span class="p">:</span> <span class="o">-</span><span class="mf">1.6</span><span class="p">,</span> <span class="s1">&#39;z&#39;</span><span class="p">:</span> <span class="mf">0.6</span><span class="p">}}},</span>
                                     <span class="n">legend</span><span class="o">=</span><span class="p">{</span><span class="s2">&quot;yanchor&quot;</span> <span class="p">:</span> <span class="s2">&quot;top&quot;</span><span class="p">,</span>
                                        <span class="s2">&quot;y&quot;</span> <span class="p">:</span> <span class="mf">0.99</span><span class="p">,</span>
                                        <span class="s2">&quot;xanchor&quot;</span> <span class="p">:</span> <span class="s2">&quot;left&quot;</span><span class="p">,</span>
                                        <span class="s2">&quot;x&quot;</span> <span class="p">:</span> <span class="mf">0.01</span><span class="p">})</span>
    <span class="k">if</span> <span class="n">y_1_or_0</span><span class="o">==</span><span class="kc">True</span><span class="p">:</span>
        <span class="n">scatter_3d</span><span class="o">.</span><span class="n">update_layout</span><span class="p">(</span><span class="n">scene</span><span class="o">=</span><span class="p">{</span><span class="s1">&#39;zaxis&#39;</span><span class="p">:</span> <span class="p">{</span><span class="s2">&quot;tickvals&quot;</span><span class="p">:[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">]}})</span>

    <span class="c1"># show the plot</span>
    <span class="n">scatter_3d</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
<section id="more-logistic-regression">
<h2>More Logistic Regression<a class="headerlink" href="#more-logistic-regression" title="Link to this heading">#</a></h2>
<p>The dataset we will use for this page is a historical social science dataset about the Titanic disaster. Information about the dataset can be found <a class="reference external" href="https://matthew-brett.github.io/cfd2020/data/titanic.html">here</a>. A description of the variables in the dataset is below:</p>
<p><code class="docutils literal notranslate"><span class="pre">name</span></code> - Passenger Name</p>
<p><code class="docutils literal notranslate"><span class="pre">gender</span></code> - Gender of Passenger</p>
<p><code class="docutils literal notranslate"><span class="pre">age</span></code> - Age of passenger</p>
<p><code class="docutils literal notranslate"><span class="pre">class</span></code> - The class passengers travelled in</p>
<p><code class="docutils literal notranslate"><span class="pre">embarked</span></code> - Point of embarkment</p>
<p><code class="docutils literal notranslate"><span class="pre">country</span></code> - Country of origin of passenger</p>
<p><code class="docutils literal notranslate"><span class="pre">fare</span></code> - Amount paid for the ticket</p>
<p><code class="docutils literal notranslate"><span class="pre">survived</span></code> - If they survived the disaster or not</p>
<p>Here the observational units are people/passengers.</p>
<p><em>Note</em>: the data has been shuffled - the original data was in alphabetical order.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># read in the data</span>
<span class="n">full_df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s1">&#39;data/titanic.csv&#39;</span><span class="p">)</span>
<span class="n">full_df</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>name</th>
      <th>gender</th>
      <th>age</th>
      <th>class</th>
      <th>embarked</th>
      <th>country</th>
      <th>fare</th>
      <th>survived</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>Angheloff, Mr. Minko</td>
      <td>male</td>
      <td>26.0</td>
      <td>3rd</td>
      <td>Southampton</td>
      <td>Bulgaria</td>
      <td>7.1711</td>
      <td>no</td>
    </tr>
    <tr>
      <th>1</th>
      <td>Louch, Mrs. Alice Adelaide</td>
      <td>female</td>
      <td>43.0</td>
      <td>2nd</td>
      <td>Southampton</td>
      <td>England</td>
      <td>26.0000</td>
      <td>yes</td>
    </tr>
    <tr>
      <th>2</th>
      <td>Sawyer, Mr. Frederick Charles</td>
      <td>male</td>
      <td>24.0</td>
      <td>3rd</td>
      <td>Southampton</td>
      <td>England</td>
      <td>8.0100</td>
      <td>no</td>
    </tr>
    <tr>
      <th>3</th>
      <td>Abbing, Mr. Anthony</td>
      <td>male</td>
      <td>42.0</td>
      <td>3rd</td>
      <td>Southampton</td>
      <td>United States</td>
      <td>7.1100</td>
      <td>no</td>
    </tr>
    <tr>
      <th>4</th>
      <td>Givard, Mr. Hans Kristensen</td>
      <td>male</td>
      <td>30.0</td>
      <td>2nd</td>
      <td>Southampton</td>
      <td>Argentina</td>
      <td>13.0000</td>
      <td>no</td>
    </tr>
    <tr>
      <th>...</th>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
    </tr>
    <tr>
      <th>1208</th>
      <td>Rosblom, Miss. Salli Helena</td>
      <td>female</td>
      <td>2.0</td>
      <td>3rd</td>
      <td>Southampton</td>
      <td>Finland</td>
      <td>20.0403</td>
      <td>no</td>
    </tr>
    <tr>
      <th>1209</th>
      <td>Goldsmith, Master. Frank John William</td>
      <td>male</td>
      <td>9.0</td>
      <td>3rd</td>
      <td>Southampton</td>
      <td>England</td>
      <td>20.1006</td>
      <td>yes</td>
    </tr>
    <tr>
      <th>1210</th>
      <td>Fortune, Mrs. Mary</td>
      <td>female</td>
      <td>60.0</td>
      <td>1st</td>
      <td>Southampton</td>
      <td>Canada</td>
      <td>263.0000</td>
      <td>yes</td>
    </tr>
    <tr>
      <th>1211</th>
      <td>Klaber, Mr. Herman</td>
      <td>male</td>
      <td>41.0</td>
      <td>1st</td>
      <td>Southampton</td>
      <td>United States</td>
      <td>26.1100</td>
      <td>no</td>
    </tr>
    <tr>
      <th>1212</th>
      <td>Andrew, Mr. Edgar Samuel</td>
      <td>male</td>
      <td>17.0</td>
      <td>2nd</td>
      <td>Southampton</td>
      <td>Argentina</td>
      <td>11.1000</td>
      <td>no</td>
    </tr>
  </tbody>
</table>
<p>1213 rows × 8 columns</p>
</div></div></div>
</div>
<p>You’ll remember that we can use logistic regression when we want to predict a binary categorical outcome variable.</p>
<p>In this case, we’ll be interested in predicting whether a passenger <code class="docutils literal notranslate"><span class="pre">survived</span></code>.</p>
<p>Currently, <code class="docutils literal notranslate"><span class="pre">survived</span></code> has two categories into which observational units can fall (<code class="docutils literal notranslate"><span class="pre">yes</span></code> or <code class="docutils literal notranslate"><span class="pre">no</span></code>).</p>
<p>We’ll dummy code these in the now familiar manner, 1 representing <code class="docutils literal notranslate"><span class="pre">yes</span></code> and 0 representing <code class="docutils literal notranslate"><span class="pre">no</span></code>:</p>
<p><code class="docutils literal notranslate"><span class="pre">survived_dummy</span></code> <span class="math notranslate nohighlight">\( = \begin{cases} 1, &amp; \text{if \)</span>y_i<span class="math notranslate nohighlight">\( == `yes`} \\ 0, &amp; \text{if \)</span>y_i<span class="math notranslate nohighlight">\( == `no`} \end{cases} \)</span></p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># dummy code &#39;survived&#39;</span>
<span class="n">full_df</span><span class="p">[</span><span class="s1">&#39;survived_dummy&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span><span class="n">full_df</span><span class="p">[</span><span class="s1">&#39;survived&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">replace</span><span class="p">([</span><span class="s1">&#39;yes&#39;</span><span class="p">,</span> <span class="s1">&#39;no&#39;</span><span class="p">],</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">])</span>
                             <span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="nb">int</span><span class="p">))</span>
<span class="n">full_df</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>name</th>
      <th>gender</th>
      <th>age</th>
      <th>class</th>
      <th>embarked</th>
      <th>country</th>
      <th>fare</th>
      <th>survived</th>
      <th>survived_dummy</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>Angheloff, Mr. Minko</td>
      <td>male</td>
      <td>26.0</td>
      <td>3rd</td>
      <td>Southampton</td>
      <td>Bulgaria</td>
      <td>7.1711</td>
      <td>no</td>
      <td>0</td>
    </tr>
    <tr>
      <th>1</th>
      <td>Louch, Mrs. Alice Adelaide</td>
      <td>female</td>
      <td>43.0</td>
      <td>2nd</td>
      <td>Southampton</td>
      <td>England</td>
      <td>26.0000</td>
      <td>yes</td>
      <td>1</td>
    </tr>
    <tr>
      <th>2</th>
      <td>Sawyer, Mr. Frederick Charles</td>
      <td>male</td>
      <td>24.0</td>
      <td>3rd</td>
      <td>Southampton</td>
      <td>England</td>
      <td>8.0100</td>
      <td>no</td>
      <td>0</td>
    </tr>
    <tr>
      <th>3</th>
      <td>Abbing, Mr. Anthony</td>
      <td>male</td>
      <td>42.0</td>
      <td>3rd</td>
      <td>Southampton</td>
      <td>United States</td>
      <td>7.1100</td>
      <td>no</td>
      <td>0</td>
    </tr>
    <tr>
      <th>4</th>
      <td>Givard, Mr. Hans Kristensen</td>
      <td>male</td>
      <td>30.0</td>
      <td>2nd</td>
      <td>Southampton</td>
      <td>Argentina</td>
      <td>13.0000</td>
      <td>no</td>
      <td>0</td>
    </tr>
    <tr>
      <th>...</th>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
    </tr>
    <tr>
      <th>1208</th>
      <td>Rosblom, Miss. Salli Helena</td>
      <td>female</td>
      <td>2.0</td>
      <td>3rd</td>
      <td>Southampton</td>
      <td>Finland</td>
      <td>20.0403</td>
      <td>no</td>
      <td>0</td>
    </tr>
    <tr>
      <th>1209</th>
      <td>Goldsmith, Master. Frank John William</td>
      <td>male</td>
      <td>9.0</td>
      <td>3rd</td>
      <td>Southampton</td>
      <td>England</td>
      <td>20.1006</td>
      <td>yes</td>
      <td>1</td>
    </tr>
    <tr>
      <th>1210</th>
      <td>Fortune, Mrs. Mary</td>
      <td>female</td>
      <td>60.0</td>
      <td>1st</td>
      <td>Southampton</td>
      <td>Canada</td>
      <td>263.0000</td>
      <td>yes</td>
      <td>1</td>
    </tr>
    <tr>
      <th>1211</th>
      <td>Klaber, Mr. Herman</td>
      <td>male</td>
      <td>41.0</td>
      <td>1st</td>
      <td>Southampton</td>
      <td>United States</td>
      <td>26.1100</td>
      <td>no</td>
      <td>0</td>
    </tr>
    <tr>
      <th>1212</th>
      <td>Andrew, Mr. Edgar Samuel</td>
      <td>male</td>
      <td>17.0</td>
      <td>2nd</td>
      <td>Southampton</td>
      <td>Argentina</td>
      <td>11.1000</td>
      <td>no</td>
      <td>0</td>
    </tr>
  </tbody>
</table>
<p>1213 rows × 9 columns</p>
</div></div></div>
</div>
<p>To keep the data easier to visualise, we’ll work with a restricted set of rows. The data has been shuffled before it was imported, so if we take the first 25 rows of the data, this constitutes a random sample, and so is likely to be representative of the whole dataset.</p>
<p>We’ll build an evaluate a variety of different models, using <code class="docutils literal notranslate"><span class="pre">survived</span></code> as our outcome variable (e.g. our <span class="math notranslate nohighlight">\(y\)</span> variable). We’ll use any (combination) of <code class="docutils literal notranslate"><span class="pre">age</span></code>, <code class="docutils literal notranslate"><span class="pre">fare</span></code> and <code class="docutils literal notranslate"><span class="pre">gender</span></code> as our predictor variables (e.g. our <span class="math notranslate nohighlight">\(x\)</span> variables).</p>
<p>Let’s trim the dataframe down to just the variables of interest:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># isolate only the variables of interest, select the first 25 rows</span>
<span class="n">sample_df</span> <span class="o">=</span> <span class="n">full_df</span><span class="o">.</span><span class="n">loc</span><span class="p">[:</span><span class="mi">25</span><span class="p">,</span> <span class="p">[</span><span class="s1">&#39;age&#39;</span><span class="p">,</span> <span class="s1">&#39;fare&#39;</span><span class="p">,</span> <span class="s1">&#39;survived&#39;</span><span class="p">,</span> <span class="s1">&#39;gender&#39;</span><span class="p">,</span> <span class="s1">&#39;survived_dummy&#39;</span><span class="p">]]</span>

<span class="n">sample_df</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>age</th>
      <th>fare</th>
      <th>survived</th>
      <th>gender</th>
      <th>survived_dummy</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>26.0</td>
      <td>7.1711</td>
      <td>no</td>
      <td>male</td>
      <td>0</td>
    </tr>
    <tr>
      <th>1</th>
      <td>43.0</td>
      <td>26.0000</td>
      <td>yes</td>
      <td>female</td>
      <td>1</td>
    </tr>
    <tr>
      <th>2</th>
      <td>24.0</td>
      <td>8.0100</td>
      <td>no</td>
      <td>male</td>
      <td>0</td>
    </tr>
    <tr>
      <th>3</th>
      <td>42.0</td>
      <td>7.1100</td>
      <td>no</td>
      <td>male</td>
      <td>0</td>
    </tr>
    <tr>
      <th>4</th>
      <td>30.0</td>
      <td>13.0000</td>
      <td>no</td>
      <td>male</td>
      <td>0</td>
    </tr>
    <tr>
      <th>5</th>
      <td>22.0</td>
      <td>7.1511</td>
      <td>no</td>
      <td>male</td>
      <td>0</td>
    </tr>
    <tr>
      <th>6</th>
      <td>19.0</td>
      <td>8.0100</td>
      <td>no</td>
      <td>male</td>
      <td>0</td>
    </tr>
    <tr>
      <th>7</th>
      <td>25.0</td>
      <td>13.0000</td>
      <td>no</td>
      <td>male</td>
      <td>0</td>
    </tr>
    <tr>
      <th>8</th>
      <td>32.0</td>
      <td>15.1000</td>
      <td>no</td>
      <td>female</td>
      <td>0</td>
    </tr>
    <tr>
      <th>9</th>
      <td>50.0</td>
      <td>65.0000</td>
      <td>yes</td>
      <td>female</td>
      <td>1</td>
    </tr>
    <tr>
      <th>10</th>
      <td>62.0</td>
      <td>10.1000</td>
      <td>yes</td>
      <td>male</td>
      <td>1</td>
    </tr>
    <tr>
      <th>11</th>
      <td>21.0</td>
      <td>7.1500</td>
      <td>no</td>
      <td>female</td>
      <td>0</td>
    </tr>
    <tr>
      <th>12</th>
      <td>26.0</td>
      <td>7.1711</td>
      <td>no</td>
      <td>male</td>
      <td>0</td>
    </tr>
    <tr>
      <th>13</th>
      <td>34.0</td>
      <td>81.1702</td>
      <td>yes</td>
      <td>female</td>
      <td>1</td>
    </tr>
    <tr>
      <th>14</th>
      <td>21.0</td>
      <td>31.1000</td>
      <td>no</td>
      <td>male</td>
      <td>0</td>
    </tr>
    <tr>
      <th>15</th>
      <td>52.0</td>
      <td>30.0000</td>
      <td>yes</td>
      <td>female</td>
      <td>1</td>
    </tr>
    <tr>
      <th>16</th>
      <td>19.0</td>
      <td>26.0508</td>
      <td>yes</td>
      <td>female</td>
      <td>1</td>
    </tr>
    <tr>
      <th>17</th>
      <td>23.0</td>
      <td>13.0000</td>
      <td>no</td>
      <td>male</td>
      <td>0</td>
    </tr>
    <tr>
      <th>18</th>
      <td>9.0</td>
      <td>46.1800</td>
      <td>no</td>
      <td>male</td>
      <td>0</td>
    </tr>
    <tr>
      <th>19</th>
      <td>22.0</td>
      <td>7.0406</td>
      <td>no</td>
      <td>male</td>
      <td>0</td>
    </tr>
    <tr>
      <th>20</th>
      <td>36.0</td>
      <td>8.0100</td>
      <td>no</td>
      <td>male</td>
      <td>0</td>
    </tr>
    <tr>
      <th>21</th>
      <td>36.0</td>
      <td>120.0000</td>
      <td>yes</td>
      <td>male</td>
      <td>1</td>
    </tr>
    <tr>
      <th>22</th>
      <td>5.0</td>
      <td>27.1800</td>
      <td>no</td>
      <td>male</td>
      <td>0</td>
    </tr>
    <tr>
      <th>23</th>
      <td>17.0</td>
      <td>7.1711</td>
      <td>no</td>
      <td>male</td>
      <td>0</td>
    </tr>
    <tr>
      <th>24</th>
      <td>1.0</td>
      <td>39.0000</td>
      <td>yes</td>
      <td>male</td>
      <td>1</td>
    </tr>
    <tr>
      <th>25</th>
      <td>30.0</td>
      <td>13.1702</td>
      <td>yes</td>
      <td>female</td>
      <td>1</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
<p>And just to save some typing, let’s store the first predictor we will use (<code class="docutils literal notranslate"><span class="pre">fare</span></code>) and the outcome variable (<code class="docutils literal notranslate"><span class="pre">survived_dummy</span></code>) as numpy arrays:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># store `fare` and `survived_dummy` as numpy arrays</span>
<span class="n">fare</span> <span class="o">=</span> <span class="n">sample_df</span><span class="p">[</span><span class="s1">&#39;fare&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">values</span>
<span class="n">survived_dummy</span> <span class="o">=</span> <span class="n">sample_df</span><span class="p">[</span><span class="s1">&#39;survived_dummy&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">values</span>
</pre></div>
</div>
</div>
</div>
</section>
<section id="our-first-logistic-model-and-two-perspectives-on-the-cost-function">
<h2>Our first logistic model - and two perspectives on the cost function<a class="headerlink" href="#our-first-logistic-model-and-two-perspectives-on-the-cost-function" title="Link to this heading">#</a></h2>
<p>We’ll now fit our first logistic regression model on this page. We’ll predict <code class="docutils literal notranslate"><span class="pre">survived</span></code> as a function of <code class="docutils literal notranslate"><span class="pre">fare</span></code>.</p>
<p>Were richer people more likely to survive the disaster?</p>
<p>As always, it’s best to do some graphical analysis first, before fitting the model:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># a convenience function to generate the scatterplot</span>
<span class="k">def</span><span class="w"> </span><span class="nf">plot_scatter</span><span class="p">(</span><span class="n">log_odds</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
    <span class="n">colors</span> <span class="o">=</span> <span class="n">sample_df</span><span class="p">[</span><span class="s1">&#39;survived_dummy&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">replace</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="p">[</span><span class="s1">&#39;red&#39;</span><span class="p">,</span> <span class="s1">&#39;blue&#39;</span><span class="p">])</span>
    <span class="c1"># Build plot, add custom labels.</span>
    <span class="k">if</span> <span class="n">log_odds</span><span class="o">==</span><span class="kc">False</span><span class="p">:</span>
        <span class="n">sample_df</span><span class="o">.</span><span class="n">plot</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="s1">&#39;fare&#39;</span><span class="p">,</span> <span class="s1">&#39;survived_dummy&#39;</span><span class="p">,</span> <span class="n">c</span><span class="o">=</span><span class="n">colors</span><span class="p">)</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;Survived</span><span class="se">\n</span><span class="s1">0 = NO, 1 = YES&#39;</span><span class="p">)</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">yticks</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">]);</span>  <span class="c1"># Just label 0 and 1 on the y axis.</span>
    <span class="k">if</span> <span class="n">log_odds</span><span class="o">==</span><span class="kc">True</span><span class="p">:</span>
        <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">({</span><span class="s1">&#39;fare&#39;</span><span class="p">:</span><span class="n">fare</span><span class="p">,</span>
                     <span class="s1">&#39;log_odds_of_survival&#39;</span><span class="p">:</span> <span class="n">log_odds_predictions_single_predictor</span><span class="p">})</span><span class="o">.</span><span class="n">plot</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="s1">&#39;fare&#39;</span><span class="p">,</span> <span class="s1">&#39;log_odds_of_survival&#39;</span><span class="p">,</span> <span class="n">c</span><span class="o">=</span><span class="n">colors</span><span class="p">)</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;Predicted Log odds of Survival&#39;</span><span class="p">)</span>
    <span class="c1"># Put a custom legend on the plot. This code is a little obscure.</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">([],</span> <span class="p">[],</span> <span class="n">c</span><span class="o">=</span><span class="s1">&#39;blue&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;NO&#39;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">([],</span> <span class="p">[],</span> <span class="n">c</span><span class="o">=</span><span class="s1">&#39;red&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;YES&#39;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>

<span class="n">plot_scatter</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/51d93d92790b3580269c686040427af15e90f202aa7b92ab9c38ccb6e7ec3775.png" src="_images/51d93d92790b3580269c686040427af15e90f202aa7b92ab9c38ccb6e7ec3775.png" />
</div>
</div>
<p>From the graphical trend, it does look as though <code class="docutils literal notranslate"><span class="pre">fare</span></code> is associated with a higher probability of survival.</p>
<p>In fact, in this sample of 25 passengers, everybody who paid above 60 survived.</p>
<p>The process of fitting our single predictor linear regression is as follows:</p>
<ul class="simple">
<li><p>we want to predict a binary categorical outcome variable (where each outcome category is dummy coded as 0 or 1)</p></li>
<li><p>we use a slope/intercept pair to generate a line (this line applies on the scale of the log of the odds)</p></li>
<li><p>we then use the inverse logit transformation to convert the line into a probability curve</p></li>
<li><p>the fit of this curve is evaluated against the actual data by finding the maximum likelihood (the line which maximizes the probability of observing the actual data)</p></li>
<li><p>(in practice, to make the numbers easier for a computer to deal with we <code class="docutils literal notranslate"><span class="pre">minimize</span></code> the negative log-likelihood)</p></li>
</ul>
<p>Let’s define our <code class="docutils literal notranslate"><span class="pre">inverse_logit()</span></code> function and our cost function:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span><span class="w"> </span><span class="nf">inverse_logit</span><span class="p">(</span><span class="n">y</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot; Convert a line on the log odds scale to a sigmoid probability curve.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">odds</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>  <span class="c1"># Reverse the log operation.</span>
    <span class="k">return</span> <span class="n">odds</span> <span class="o">/</span> <span class="p">(</span><span class="n">odds</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span>  <span class="c1"># Reverse odds operation.</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span><span class="w"> </span><span class="nf">mll_logit_cost_one_predictor</span><span class="p">(</span><span class="n">intercept_and_slope</span><span class="p">,</span> <span class="n">x1</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot; Cost function for maximum log likelihood</span>

<span class="sd">    Return minus of the log of the likelihood.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># Unpack the intercept and slope</span>
    <span class="n">intercept</span><span class="p">,</span> <span class="n">slope_1</span> <span class="o">=</span> <span class="n">intercept_and_slope</span>
    
    <span class="c1"># Make predictions for on the log odds (straight line) scale.</span>
    <span class="n">predicted_log_odds</span> <span class="o">=</span> <span class="n">intercept</span> <span class="o">+</span> <span class="n">slope_1</span> <span class="o">*</span> <span class="n">x1</span> 

    <span class="c1"># convert these predictions to sigmoid probability predictions</span>
    <span class="n">predicted_prob_of_1</span> <span class="o">=</span> <span class="n">inverse_logit</span><span class="p">(</span><span class="n">predicted_log_odds</span><span class="p">)</span>

    <span class="c1"># Calculate predicted probabilities of the actual outcome category for each observation.</span>
    <span class="n">predicted_probability_of_actual_score</span> <span class="o">=</span> <span class="n">y</span> <span class="o">*</span> <span class="n">predicted_prob_of_1</span> <span class="o">+</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">y</span><span class="p">)</span> <span class="o">*</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">predicted_prob_of_1</span><span class="p">)</span>
    
    <span class="c1"># Use logs to calculate log of the likelihood</span>
    <span class="n">log_likelihood</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">predicted_probability_of_actual_score</span><span class="p">))</span>
    
    <span class="c1"># Ask minimize to find maximum by adding minus sign.</span>
    <span class="k">return</span> <span class="o">-</span><span class="n">log_likelihood</span>
</pre></div>
</div>
</div>
</div>
<p>We can pass our cost function, along with an initial guess at the slope and intercept, to <code class="docutils literal notranslate"><span class="pre">minimize</span></code>:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">logistic_reg_ML_one_pred</span> <span class="o">=</span> <span class="n">minimize</span><span class="p">(</span><span class="n">mll_logit_cost_one_predictor</span><span class="p">,</span>  <span class="c1"># Cost function</span>
                 <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">],</span>  <span class="c1"># Guessed intercept and slope</span>
                 <span class="n">args</span><span class="o">=</span><span class="p">(</span><span class="n">fare</span><span class="p">,</span> <span class="n">survived_dummy</span><span class="p">),</span>  <span class="c1"># x and y values</span>
                 <span class="n">tol</span><span class="o">=</span><span class="mf">1e-20</span><span class="p">)</span>  <span class="c1"># Attend to tiny changes in cost function values.</span>
<span class="c1"># Show the result.</span>
<span class="n">logistic_reg_ML_one_pred</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>  message: Optimization terminated successfully.
  success: True
   status: 0
      fun: 11.56520905235432
        x: [-2.541e+00  8.280e-02]
      nit: 9
      jac: [ 0.000e+00  0.000e+00]
 hess_inv: [[ 8.814e-01 -3.008e-02]
            [-3.008e-02  1.477e-03]]
     nfev: 33
     njev: 11
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># show just the intercept and slope</span>
<span class="n">logistic_reg_ML_one_pred</span><span class="o">.</span><span class="n">x</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>array([-2.54144967,  0.08279907])
</pre></div>
</div>
</div>
</div>
<p>As before, let’s check how <code class="docutils literal notranslate"><span class="pre">minimize</span></code> did against <code class="docutils literal notranslate"><span class="pre">statsmodels</span></code>:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Create the model.</span>
<span class="n">log_reg_mod_single_predictor</span> <span class="o">=</span> <span class="n">smf</span><span class="o">.</span><span class="n">logit</span><span class="p">(</span><span class="s1">&#39;survived_dummy ~ fare&#39;</span><span class="p">,</span> <span class="n">data</span><span class="o">=</span><span class="n">sample_df</span><span class="p">)</span>
<span class="c1"># Fit it.</span>
<span class="n">fitted_log_reg_mod_single_predictor</span> <span class="o">=</span> <span class="n">log_reg_mod_single_predictor</span><span class="o">.</span><span class="n">fit</span><span class="p">()</span>
<span class="n">fitted_log_reg_mod_single_predictor</span><span class="o">.</span><span class="n">summary</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Optimization terminated successfully.
         Current function value: 0.444816
         Iterations 7
</pre></div>
</div>
<div class="output text_html"><table class="simpletable">
<caption>Logit Regression Results</caption>
<tr>
  <th>Dep. Variable:</th>    <td>survived_dummy</td>  <th>  No. Observations:  </th>  <td>    26</td> 
</tr>
<tr>
  <th>Model:</th>                 <td>Logit</td>      <th>  Df Residuals:      </th>  <td>    24</td> 
</tr>
<tr>
  <th>Method:</th>                 <td>MLE</td>       <th>  Df Model:          </th>  <td>     1</td> 
</tr>
<tr>
  <th>Date:</th>            <td>Wed, 28 Jan 2026</td> <th>  Pseudo R-squ.:     </th>  <td>0.3104</td> 
</tr>
<tr>
  <th>Time:</th>                <td>11:43:32</td>     <th>  Log-Likelihood:    </th> <td> -11.565</td>
</tr>
<tr>
  <th>converged:</th>             <td>True</td>       <th>  LL-Null:           </th> <td> -16.771</td>
</tr>
<tr>
  <th>Covariance Type:</th>     <td>nonrobust</td>    <th>  LLR p-value:       </th> <td>0.001252</td>
</tr>
</table>
<table class="simpletable">
<tr>
      <td></td>         <th>coef</th>     <th>std err</th>      <th>z</th>      <th>P>|z|</th>  <th>[0.025</th>    <th>0.975]</th>  
</tr>
<tr>
  <th>Intercept</th> <td>   -2.5415</td> <td>    0.944</td> <td>   -2.691</td> <td> 0.007</td> <td>   -4.392</td> <td>   -0.691</td>
</tr>
<tr>
  <th>fare</th>      <td>    0.0828</td> <td>    0.039</td> <td>    2.146</td> <td> 0.032</td> <td>    0.007</td> <td>    0.158</td>
</tr>
</table></div></div>
</div>
<p>We can see that the estimates are the same between both methods.</p>
<p>Remember that the (log) odds and probabilities relate to the the outcome category which we have dummy coded as 1.
So the <code class="docutils literal notranslate"><span class="pre">fare</span></code> slope of <code class="docutils literal notranslate"><span class="pre">0.0828</span></code> means that for every 1-unit increase in <code class="docutils literal notranslate"><span class="pre">fare</span></code> we predict a <code class="docutils literal notranslate"><span class="pre">0.0828</span></code>
increase in the log odds of survival.</p>
<p>We can convert this to an odds ratio using <code class="docutils literal notranslate"><span class="pre">np.exp</span></code>:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">fare_odds_ratio</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">fitted_log_reg_mod_single_predictor</span><span class="o">.</span><span class="n">params</span><span class="p">[</span><span class="s1">&#39;fare&#39;</span><span class="p">])</span>
<span class="n">fare_odds_ratio</span> 
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>np.float64(1.086323539461138)
</pre></div>
</div>
</div>
</div>
<p>The odds ratio is a multiplier which describes odds change for a 1-unit increase in the predictor variable:</p>
<p><span class="math notranslate nohighlight">\(\Large e^{b_1} = \frac{\text{odds(survival) at } x + 1}{\text{odds(survival) at } x}\)</span></p>
<p>We can make this more intuitive by converting it to a percentage change in odds, for a 1-unit increase in the predictor, using the following formula:</p>
<p><span class="math notranslate nohighlight">\(\text{Percent Change in the Odds=(Odds Ratio−1)×100}\)</span></p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="p">(</span><span class="n">fare_odds_ratio</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span> <span class="o">*</span> <span class="mi">100</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>np.float64(8.632353946113792)
</pre></div>
</div>
</div>
</div>
<p>This means the odds of survival increase by about 8.63% for every 1-unit increase in <code class="docutils literal notranslate"><span class="pre">fare</span></code>.
Meaning that richer people were more likely to survive the disaster.</p>
<p>We can use the parameters to generate probability predictions for each observational unit,
conditional on their <code class="docutils literal notranslate"><span class="pre">fare</span></code> score. The probability here is the probability of survival (e.g. the outcome category which we dummy coded as 1).
We can do this using our <code class="docutils literal notranslate"><span class="pre">inverse_logit</span></code> function.</p>
<p>So logistic regression in the present case tells us:</p>
<ul class="simple">
<li><p>the log odds of survival, conditional on <code class="docutils literal notranslate"><span class="pre">fare</span></code></p></li>
<li><p>the odds of survival, conditional on <code class="docutils literal notranslate"><span class="pre">fare</span></code></p></li>
<li><p>the probability of survival, conditional on <code class="docutils literal notranslate"><span class="pre">fare</span></code></p></li>
</ul>
<p>After fitting the model, the parameters come to us on the log odds scale. The
mathematical notation for the log odds predictions is:</p>
<p><span class="math notranslate nohighlight">\( \text{log odds}(y_i == 1) = b_0 + b_1 * x_1\)</span></p>
<p>Which in pythonic terms is:</p>
<p><span class="math notranslate nohighlight">\( \text{log odds of survival} = b_0 + b_1 * \)</span> <code class="docutils literal notranslate"><span class="pre">fare</span></code></p>
<p>Let’s generate these predictions from our parameters:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># isolate the parameters from the model</span>
<span class="n">intercept_single_predictor</span> <span class="o">=</span> <span class="n">fitted_log_reg_mod_single_predictor</span><span class="o">.</span><span class="n">params</span><span class="p">[</span><span class="s1">&#39;Intercept&#39;</span><span class="p">]</span>
<span class="n">fare_slope_single_predictor</span> <span class="o">=</span> <span class="n">fitted_log_reg_mod_single_predictor</span><span class="o">.</span><span class="n">params</span><span class="p">[</span><span class="s1">&#39;fare&#39;</span><span class="p">]</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># generate the predictions</span>
<span class="n">log_odds_predictions_single_predictor</span> <span class="o">=</span> <span class="n">intercept_single_predictor</span> <span class="o">+</span> <span class="n">fare_slope_single_predictor</span> <span class="o">*</span> <span class="n">fare</span>
<span class="n">log_odds_predictions_single_predictor</span> 
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>array([-1.94768955, -0.38867366, -1.87822939, -1.95274858, -1.46506191,
       -1.94934554, -1.87822939, -1.46506191, -1.29118381,  2.84049107,
       -1.70517928, -1.94943662, -1.94768955,  4.179369  ,  0.03360172,
       -0.05747728, -0.38446747, -1.46506191,  1.28221209, -1.95849484,
       -1.87822939,  7.39444132, -0.29097073, -1.94768955,  0.68771458,
       -1.4509695 ])
</pre></div>
</div>
</div>
</div>
<p>We can plot these predictions, and, as expected, see that they form a straight line:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># generate a plot of the predicted log odds of survival</span>
<span class="n">intercept_single_predictor</span> <span class="o">=</span> <span class="n">fitted_log_reg_mod_single_predictor</span><span class="o">.</span><span class="n">params</span><span class="p">[</span><span class="s1">&#39;Intercept&#39;</span><span class="p">]</span>
<span class="n">fare_slope_single_predictor</span> <span class="o">=</span> <span class="n">fitted_log_reg_mod_single_predictor</span><span class="o">.</span><span class="n">params</span><span class="p">[</span><span class="s1">&#39;fare&#39;</span><span class="p">]</span>
<span class="n">fine_x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">min</span><span class="p">(</span><span class="n">fare</span><span class="p">),</span> <span class="n">np</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">fare</span><span class="p">),</span> <span class="mi">1000</span><span class="p">)</span>
<span class="n">fine_log_odds_predictions_single_predictor</span> <span class="o">=</span> <span class="n">intercept_single_predictor</span> <span class="o">+</span> <span class="n">fare_slope_single_predictor</span><span class="o">*</span><span class="n">fine_x</span>
<span class="n">plot_scatter</span><span class="p">(</span><span class="n">log_odds</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">fine_x</span><span class="p">,</span> <span class="n">fine_log_odds_predictions_single_predictor</span> <span class="p">,</span>  <span class="n">linewidth</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="s1">&#39;:&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;Predicted Log Odds of Survival&#39;</span><span class="p">);</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/e376234b8f94840aefa74e2323dca34d544d6dc350139b3df2c36dd704d9d590.png" src="_images/e376234b8f94840aefa74e2323dca34d544d6dc350139b3df2c36dd704d9d590.png" />
</div>
</div>
<p>We can use the inverse logit transformation to convert these log odds predictions to probabilities.</p>
<p>We first convert them to odds, and then convert the odds to probabilities. Again, this is the
predicted probability of an observational unit scoring <code class="docutils literal notranslate"><span class="pre">1</span></code> on the outcome variable (which
in this case means surviving).</p>
<p>The mathematical notation for this transformation is:</p>
<p><span class="math notranslate nohighlight">\( \Large \text{odds}(y_i == 1) = e^{b_{0} + b_{1} * x_{1}}\)</span></p>
<p><span class="math notranslate nohighlight">\( \Large \text{prob}(y_i == 1) = \frac{e^{b_{0} + b_{1} * x_{1}}}{1 + e^{b_{0} + b_{1} * x_{1}}}\)</span></p>
<p>This is simpler in python!:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># shown in pythonic form again, for convenience</span>
<span class="k">def</span><span class="w"> </span><span class="nf">inverse_logit</span><span class="p">(</span><span class="n">y</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot; Reverse logit transformation</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">odds</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>  <span class="c1"># Reverse the log operation.</span>
    <span class="k">return</span> <span class="n">odds</span> <span class="o">/</span> <span class="p">(</span><span class="n">odds</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span>  <span class="c1"># Reverse odds operation.</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># convert the log odds predictions to probabilities</span>
<span class="n">probability_predictions_single_predictor</span> <span class="o">=</span> <span class="n">inverse_logit</span><span class="p">(</span><span class="n">log_odds_predictions_single_predictor</span><span class="p">)</span>
<span class="n">probability_predictions_single_predictor</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>array([0.12480551, 0.40403663, 0.13259238, 0.12425396, 0.18769434,
       0.12462474, 0.13259238, 0.18769434, 0.21565251, 0.94482507,
       0.15379003, 0.1246148 , 0.12480551, 0.98492264, 0.50839964,
       0.48563463, 0.40504985, 0.18769434, 0.78282609, 0.12363003,
       0.13259238, 0.99938572, 0.42776623, 0.12480551, 0.66545833,
       0.1898524 ])
</pre></div>
</div>
</div>
</div>
<p>If we plot the probability predictions as a function of <code class="docutils literal notranslate"><span class="pre">fare</span></code>, we see the
sigmoid (s-shaped) probability curve, so us the predicted probability of survival
at each value of <code class="docutils literal notranslate"><span class="pre">fare</span></code>:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># generate the plot</span>
<span class="n">fine_prob_predictions</span> <span class="o">=</span> <span class="n">inverse_logit</span><span class="p">(</span><span class="n">fine_log_odds_predictions_single_predictor</span><span class="p">)</span>
<span class="n">plot_scatter</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">fine_x</span><span class="p">,</span> <span class="n">fine_prob_predictions</span><span class="p">,</span>  <span class="n">linewidth</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="s1">&#39;:&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">fare</span><span class="p">,</span> <span class="n">probability_predictions_single_predictor</span><span class="p">,</span> 
            <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Predicted Probability of Survival&#39;</span><span class="p">,</span>
            <span class="n">color</span><span class="o">=</span><span class="s1">&#39;gold&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="p">(</span><span class="mf">1.1</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;Predicted Probability of Survival&#39;</span><span class="p">);</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/655318323c45b0caa25517822ae1d601c009f9777dcaddc6f3cb01029048cb0a.png" src="_images/655318323c45b0caa25517822ae1d601c009f9777dcaddc6f3cb01029048cb0a.png" />
</div>
</div>
<p>It is on this scale (between 0 and 1, the scale of the actual data) that the fit of
a given slope/intercept pair is evaluated.</p>
<p>We went through the mechanics of this on the Logistic Regression page, but just for some
extra clarity we will go briefly through it graphically now.</p>
<p>Because the outcome is binary, the logistic regression model implicitly fits <em>two</em> sigmoid probability curves.</p>
<p>The one we see above is the predicted probability of survival.</p>
<p>Because our outcome variable is binary-categorical, we can calculate the predicted probability of death with:</p>
<p><span class="math notranslate nohighlight">\(\text{P(Death) = 1 - P(Survival)} \)</span></p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># calculate the predicted probability of death</span>
<span class="n">probability_of_death</span> <span class="o">=</span> <span class="mi">1</span> <span class="o">-</span> <span class="n">probability_predictions_single_predictor</span>
<span class="n">probability_of_death</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>array([8.75194493e-01, 5.95963370e-01, 8.67407619e-01, 8.75746037e-01,
       8.12305660e-01, 8.75375262e-01, 8.67407619e-01, 8.12305660e-01,
       7.84347493e-01, 5.51749326e-02, 8.46209969e-01, 8.75385198e-01,
       8.75194493e-01, 1.50773573e-02, 4.91600359e-01, 5.14365366e-01,
       5.94950149e-01, 8.12305660e-01, 2.17173913e-01, 8.76369967e-01,
       8.67407619e-01, 6.14282403e-04, 5.72233767e-01, 8.75194493e-01,
       3.34541669e-01, 8.10147596e-01])
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># generate the plot</span>
<span class="n">plot_scatter</span><span class="p">()</span>
<span class="n">fine_prob_predictions_death</span> <span class="o">=</span> <span class="mi">1</span><span class="o">-</span> <span class="n">inverse_logit</span><span class="p">(</span><span class="n">fine_log_odds_predictions_single_predictor</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">fine_x</span><span class="p">,</span> <span class="n">fine_prob_predictions_death</span><span class="p">,</span>  <span class="n">linewidth</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="s1">&#39;:&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">fare</span><span class="p">,</span> <span class="n">probability_of_death</span><span class="p">,</span> 
            <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Predicted Probability of Death&#39;</span><span class="p">,</span>
            <span class="n">color</span><span class="o">=</span><span class="s1">&#39;Silver&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="p">(</span><span class="mf">1.1</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;Predicted Probability of Death&#39;</span><span class="p">);</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/6dc7d833637e351d49ac448ca555de80fd0890b3a8293e775d3a7881f7fd08de.png" src="_images/6dc7d833637e351d49ac448ca555de80fd0890b3a8293e775d3a7881f7fd08de.png" />
</div>
</div>
<p>This applies more generally: the model naturally provides predictions for the probability
of whichever outcome we coded as 1. But we can get the predicted probability of the outcome being 0 using the subtraction just shown.</p>
<p>We can actually show both of these probability curves (the probability of survival, and the probability of death)
on the same graph. The specific predicted probability for each observational unit (passenger) is show as either a silver or gold dot on the respective probability curve (gold for survival, silver for death):</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># generate the plot</span>
<span class="n">plot_scatter</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">fine_x</span><span class="p">,</span> <span class="n">fine_prob_predictions</span><span class="p">,</span>  <span class="n">linewidth</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="s1">&#39;:&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">fare</span><span class="p">,</span> <span class="n">probability_predictions_single_predictor</span><span class="p">,</span> 
            <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Predicted Probability of Survival&#39;</span><span class="p">,</span>
            <span class="n">color</span><span class="o">=</span><span class="s1">&#39;gold&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;Predicted Probability of Survival&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">fine_x</span><span class="p">,</span> <span class="n">fine_prob_predictions_death</span><span class="p">,</span>  <span class="n">linewidth</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="s1">&#39;:&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">fare</span><span class="p">,</span> <span class="n">probability_of_death</span><span class="p">,</span> 
            <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Predicted Probability of Death&#39;</span><span class="p">,</span>
            <span class="n">color</span><span class="o">=</span><span class="s1">&#39;Silver&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="p">(</span><span class="mf">1.1</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;Predicted Probability of Survival/Death&#39;</span><span class="p">);</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/b74bfa727f464f9388d5a783a1f1b5fa7c485bf891a810a0f5cd8c182db157ba.png" src="_images/b74bfa727f464f9388d5a783a1f1b5fa7c485bf891a810a0f5cd8c182db157ba.png" />
</div>
</div>
<p>These two curves provide a graphical explanation of how the logistic regression
cost function works.</p>
<p>For each observational unit (passenger), we have two predicted probabilities: <span class="math notranslate nohighlight">\(\text{P(Survival)}\)</span> and <span class="math notranslate nohighlight">\(\text{P(Death)}\)</span></p>
<p>Each passenger also has an actual <code class="docutils literal notranslate"><span class="pre">survived</span></code> score.</p>
<p>If the passenger survived, then <span class="math notranslate nohighlight">\(\text{P(Survival)}\)</span> <em>matches</em> their actual score.</p>
<p>If the passenger died then <span class="math notranslate nohighlight">\(\text{P(Death)}\)</span> <em>matches</em> their actual score.</p>
<p>Let’s call these “matching probabilities”.</p>
<p>We can show only the matching probability predictions on the graph (compare it to the graph above - you’ll see there is now
only one probability prediction per observational unit):</p>
<p>If the passenger survived, then the graph shows the predicted probability <span class="math notranslate nohighlight">\(\text{P(Survival)}\)</span> which <em>matches</em> their actual score.</p>
<p>If the passenger died, then the graph shows the predicted probability <span class="math notranslate nohighlight">\(\text{P(Death)}\)</span> which <em>matches</em> their actual score.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># generate the plot</span>
<span class="n">plot_scatter</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">fine_x</span><span class="p">,</span> <span class="n">fine_prob_predictions</span><span class="p">,</span>  <span class="n">linewidth</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="s1">&#39;:&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">fare</span><span class="p">[</span><span class="n">survived_dummy</span><span class="o">==</span><span class="mi">1</span><span class="p">],</span> <span class="n">probability_predictions_single_predictor</span><span class="p">[</span><span class="n">survived_dummy</span><span class="o">==</span><span class="mi">1</span><span class="p">],</span> 
            <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Predicted Probability of Survival&#39;</span><span class="p">,</span>
            <span class="n">color</span><span class="o">=</span><span class="s1">&#39;gold&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;Predicted Probability of Survival&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">fine_x</span><span class="p">,</span> <span class="n">fine_prob_predictions_death</span><span class="p">,</span>  <span class="n">linewidth</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="s1">&#39;:&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">fare</span><span class="p">[</span><span class="n">survived_dummy</span><span class="o">==</span><span class="mi">0</span><span class="p">],</span> <span class="n">probability_of_death</span><span class="p">[</span><span class="n">survived_dummy</span><span class="o">==</span><span class="mi">0</span><span class="p">],</span> 
            <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Predicted Probability of Death&#39;</span><span class="p">,</span>
            <span class="n">color</span><span class="o">=</span><span class="s1">&#39;Silver&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="p">(</span><span class="mf">1.1</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;Predicted Probability of Survival/Death&#39;</span><span class="p">);</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/cd59087eb5de98654b2270fc748b7d285067865daaac54084e52628087eae4f1.png" src="_images/cd59087eb5de98654b2270fc748b7d285067865daaac54084e52628087eae4f1.png" />
</div>
</div>
<p>This is how the fit of the of the current slope/intercept pair is evaluated:</p>
<ul class="simple">
<li><p>we multiply together the <em>matching</em> probabilities. We want the result to be high, meaning that
the predicted probability of each passenger’s <code class="docutils literal notranslate"><span class="pre">survived</span></code> score is high.</p></li>
<li><p>this is why the fitting technique is referred to as <em>maximum likelihood</em></p></li>
</ul>
<p>We compare the likelihood given by different slope/intercept pairs, and see which pair generates the best-fitting sigmoid curve.</p>
<p>However, multiplying together lots of very small numbers can be difficult for a computer to deal with.
In practice, to make the computations easier for a computer, we <code class="docutils literal notranslate"><span class="pre">minimize</span></code> the negative log-likelihood. For each
observational unit, the log-likelihood is:</p>
<p><span class="math notranslate nohighlight">\(\text{If the passenger survived, it is np.log(P(Survived))}\)</span></p>
<p><span class="math notranslate nohighlight">\(\text{If the passenger died, it is np.log(P(Death))}\)</span></p>
<p>We then add these log-likelihoods together, and take the negative of the result (e.g. we just add a minus sign/multiply by minus 1).</p>
<p>This gives us the same parameters as finding the maximum likelihood, but is much easier for a computer to work with.</p>
<p>The negative log-likelihood is also called the <em>log loss</em> and we can think of it (loosely) as a type of prediction error.
The graphs below so what the log loss score for a given passenger. The graph of the left shows the log loss if the passenger survived; the graph on the right shows the log loss if the passenger died:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># generate the plot</span>
<span class="n">np</span><span class="o">.</span><span class="n">seterr</span><span class="p">(</span><span class="n">divide</span> <span class="o">=</span> <span class="s1">&#39;ignore&#39;</span><span class="p">)</span> 
<span class="n">predicted_illustration_y</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="mf">0.001</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="n">loss_if_actual_is_1</span> <span class="o">=</span> <span class="o">-</span><span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">predicted_illustration_y</span><span class="p">)</span>
<span class="n">loss_if_actual_is_0</span> <span class="o">=</span> <span class="o">-</span><span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="mi">1</span><span class="o">-</span><span class="n">predicted_illustration_y</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">12</span><span class="p">,</span> <span class="mi">4</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">predicted_illustration_y</span><span class="p">,</span> <span class="n">loss_if_actual_is_1</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;red&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;Predicted Probability of Survival&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;Actual Outcome == Survived</span><span class="se">\n</span><span class="s1"> $y_i$ == 1&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;Log Loss</span><span class="se">\n</span><span class="s1"> -np.log(P)&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">predicted_illustration_y</span><span class="p">,</span> <span class="n">loss_if_actual_is_0</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;blue&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;Actual Outcome == Died</span><span class="se">\n</span><span class="s1"> $y_i$ == 0&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;Predicted Probability of Death&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;Log Loss</span><span class="se">\n</span><span class="s1"> -np.log(P)&#39;</span><span class="p">);</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/f17d8839f235200fa9116a232bfb4e48f8fe3dcad30ca313f053a811110292ef.png" src="_images/f17d8839f235200fa9116a232bfb4e48f8fe3dcad30ca313f053a811110292ef.png" />
</div>
</div>
<p>We can see that the log loss is high (high prediction error) if the predicted probability does not match the passengers actual <code class="docutils literal notranslate"><span class="pre">survived</span></code> score.</p>
<p>Likelihood and log loss are different perspectives/transformations of the same thing:</p>
<ul class="simple">
<li><p>we want likelihood to be high</p></li>
<li><p>we want log loss to be low</p></li>
</ul>
</section>
<section id="logistic-regression-in-3d-i-e-with-two-predictors">
<h2>Logistic regression in 3D (i.e. with two predictors)<a class="headerlink" href="#logistic-regression-in-3d-i-e-with-two-predictors" title="Link to this heading">#</a></h2>
<p>Now that we’ve found the best-fitting sigmoid for our single predictor model, let’s
investigate including multiple predictors in a logistic regression model.</p>
<p>We will then look at some ways of comparing these models, to see if the extra predictor is
adding anything useful.</p>
<p>The model we will now fit is <code class="docutils literal notranslate"><span class="pre">survived</span> <span class="pre">~</span> <span class="pre">fare</span> <span class="pre">+</span> <span class="pre">age</span></code>.</p>
<p>To save some typing, let’s store <code class="docutils literal notranslate"><span class="pre">age</span></code> as a separate variable:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># store age as a separate variable</span>
<span class="n">age</span> <span class="o">=</span> <span class="n">sample_df</span><span class="p">[</span><span class="s1">&#39;age&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">values</span>
</pre></div>
</div>
</div>
</div>
<p>As always, let’s graphically inspect the data, before fitting another model:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># generate the plot</span>
<span class="n">px</span><span class="o">.</span><span class="n">scatter_3d</span><span class="p">(</span><span class="n">sample_df</span><span class="p">,</span> <span class="s1">&#39;fare&#39;</span><span class="p">,</span><span class="s1">&#39;age&#39;</span><span class="p">,</span> <span class="s1">&#39;survived_dummy&#39;</span><span class="p">,</span> <span class="n">hover_data</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;survived&#39;</span><span class="p">],</span>
                               <span class="n">symbol</span><span class="o">=</span><span class="s1">&#39;survived&#39;</span><span class="p">,</span>
                               <span class="n">color</span><span class="o">=</span><span class="s1">&#39;survived&#39;</span><span class="p">,</span>
                              <span class="n">symbol_map</span><span class="o">=</span><span class="p">{</span><span class="mi">1</span><span class="p">:</span><span class="s1">&#39;x&#39;</span><span class="p">,</span> <span class="mi">0</span><span class="p">:</span><span class="s1">&#39;o&#39;</span><span class="p">})</span><span class="o">.</span><span class="n">update_layout</span><span class="p">(</span><span class="n">scene</span><span class="o">=</span><span class="p">{</span><span class="s1">&#39;zaxis&#39;</span><span class="p">:</span> <span class="p">{</span><span class="s2">&quot;tickvals&quot;</span><span class="p">:[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">]}})</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
</div>
</div>
<p>We can modify our cost function to accept two predictors. To do this, we just
add some extra arguments, and include the new predictor and its slope in the
calculation of the predictor log odds scores.</p>
<p>The rest of the cost function is the same:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span><span class="w"> </span><span class="nf">mll_logit_cost</span><span class="p">(</span><span class="n">intercept_and_slopes</span><span class="p">,</span> <span class="n">x1</span><span class="p">,</span> <span class="n">x2</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot; Cost function for maximum log likelihood</span>

<span class="sd">    Return minus of the log of the likelihood.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">intercept</span><span class="p">,</span> <span class="n">slope_1</span><span class="p">,</span> <span class="n">slope_2</span> <span class="o">=</span> <span class="n">intercept_and_slopes</span>
    
    <span class="c1"># Make predictions for on the log odds (straight line) scale.</span>
    <span class="n">predicted_log_odds</span> <span class="o">=</span> <span class="n">intercept</span> <span class="o">+</span> <span class="n">slope_1</span> <span class="o">*</span> <span class="n">x1</span> <span class="o">+</span> <span class="n">slope_2</span> <span class="o">*</span> <span class="n">x2</span>

    <span class="c1"># convert these predictions to sigmoid probability predictions</span>
    <span class="n">predicted_prob_of_1</span> <span class="o">=</span> <span class="n">inverse_logit</span><span class="p">(</span><span class="n">predicted_log_odds</span><span class="p">)</span>

    <span class="c1"># Calculate predicted probabilities of the actual score, for each observation.</span>
    <span class="n">predicted_probability_of_actual_score</span> <span class="o">=</span> <span class="n">y</span> <span class="o">*</span> <span class="n">predicted_prob_of_1</span> <span class="o">+</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">y</span><span class="p">)</span> <span class="o">*</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">predicted_prob_of_1</span><span class="p">)</span>
    
    <span class="c1"># Use logs to calculate log of the likelihood</span>
    <span class="n">log_likelihood</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">predicted_probability_of_actual_score</span><span class="p">))</span>
    
    <span class="c1"># Ask minimize to find maximum by adding minus sign.</span>
    <span class="k">return</span> <span class="o">-</span><span class="n">log_likelihood</span>
</pre></div>
</div>
</div>
</div>
<p>We then use minimize to find the best fitting parameters (slopes/intercept):</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># use minimize to find the best fitting parameters (slopes/intercept)</span>
<span class="n">logistic_reg_ML</span> <span class="o">=</span> <span class="n">minimize</span><span class="p">(</span><span class="n">mll_logit_cost</span><span class="p">,</span>  <span class="c1"># Cost function</span>
                 <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">],</span>  <span class="c1"># Guessed intercept and slope</span>
                 <span class="n">args</span><span class="o">=</span><span class="p">(</span><span class="n">fare</span><span class="p">,</span> <span class="n">age</span><span class="p">,</span> <span class="n">survived_dummy</span><span class="p">),</span>  <span class="c1"># x and y values</span>
                 <span class="n">tol</span><span class="o">=</span><span class="mf">1e-20</span><span class="p">)</span>  <span class="c1"># Attend to tiny changes in cost function values.</span>
<span class="c1"># Show the result.</span>
<span class="n">logistic_reg_ML</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>/opt/hostedtoolcache/Python/3.12.12/x64/lib/python3.12/site-packages/scipy/optimize/_numdiff.py:686: RuntimeWarning:

invalid value encountered in subtract
</pre></div>
</div>
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>  message: Desired error not necessarily achieved due to precision loss.
  success: False
   status: 2
      fun: 8.236017168397549
        x: [-6.771e+00  1.446e-01  1.101e-01]
      nit: 20
      jac: [ 0.000e+00  1.192e-07  0.000e+00]
 hess_inv: [[ 5.789e-03 -7.723e-04 -1.484e-04]
            [-7.723e-04  1.223e-03 -5.887e-04]
            [-1.484e-04 -5.887e-04  6.904e-04]]
     nfev: 258
     njev: 61
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># show just the intercept and slopes</span>
<span class="n">logistic_reg_ML</span><span class="o">.</span><span class="n">x</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>array([-6.77086462,  0.14459204,  0.11009851])
</pre></div>
</div>
</div>
</div>
<p>We can again compare those parameters to <code class="docutils literal notranslate"><span class="pre">statsmodels</span></code>.</p>
<p>We will then plot the probability predictions.</p>
<p><strong>Question:</strong> how do you think the plot will look, in 3D?</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Create the model.</span>
<span class="n">log_reg_mod</span> <span class="o">=</span> <span class="n">smf</span><span class="o">.</span><span class="n">logit</span><span class="p">(</span><span class="s1">&#39;survived_dummy ~ fare + age&#39;</span><span class="p">,</span> <span class="n">data</span><span class="o">=</span><span class="n">sample_df</span><span class="p">)</span>
<span class="c1"># Fit it.</span>
<span class="n">fitted_log_reg_mod</span> <span class="o">=</span> <span class="n">log_reg_mod</span><span class="o">.</span><span class="n">fit</span><span class="p">()</span>
<span class="n">fitted_log_reg_mod</span><span class="o">.</span><span class="n">summary</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Optimization terminated successfully.
         Current function value: 0.316770
         Iterations 8
</pre></div>
</div>
<div class="output text_html"><table class="simpletable">
<caption>Logit Regression Results</caption>
<tr>
  <th>Dep. Variable:</th>    <td>survived_dummy</td>  <th>  No. Observations:  </th>  <td>    26</td>  
</tr>
<tr>
  <th>Model:</th>                 <td>Logit</td>      <th>  Df Residuals:      </th>  <td>    23</td>  
</tr>
<tr>
  <th>Method:</th>                 <td>MLE</td>       <th>  Df Model:          </th>  <td>     2</td>  
</tr>
<tr>
  <th>Date:</th>            <td>Wed, 28 Jan 2026</td> <th>  Pseudo R-squ.:     </th>  <td>0.5089</td>  
</tr>
<tr>
  <th>Time:</th>                <td>11:43:35</td>     <th>  Log-Likelihood:    </th> <td> -8.2360</td> 
</tr>
<tr>
  <th>converged:</th>             <td>True</td>       <th>  LL-Null:           </th> <td> -16.771</td> 
</tr>
<tr>
  <th>Covariance Type:</th>     <td>nonrobust</td>    <th>  LLR p-value:       </th> <td>0.0001965</td>
</tr>
</table>
<table class="simpletable">
<tr>
      <td></td>         <th>coef</th>     <th>std err</th>      <th>z</th>      <th>P>|z|</th>  <th>[0.025</th>    <th>0.975]</th>  
</tr>
<tr>
  <th>Intercept</th> <td>   -6.7709</td> <td>    2.602</td> <td>   -2.602</td> <td> 0.009</td> <td>  -11.871</td> <td>   -1.671</td>
</tr>
<tr>
  <th>fare</th>      <td>    0.1446</td> <td>    0.065</td> <td>    2.223</td> <td> 0.026</td> <td>    0.017</td> <td>    0.272</td>
</tr>
<tr>
  <th>age</th>       <td>    0.1101</td> <td>    0.055</td> <td>    2.009</td> <td> 0.045</td> <td>    0.003</td> <td>    0.218</td>
</tr>
</table></div></div>
</div>
<p>Let’s isolate the slopes and intercept as separate python variables:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">x1_slope</span> <span class="o">=</span> <span class="n">fitted_log_reg_mod</span><span class="o">.</span><span class="n">params</span><span class="p">[</span><span class="s1">&#39;fare&#39;</span><span class="p">]</span>
<span class="n">x2_slope</span> <span class="o">=</span> <span class="n">fitted_log_reg_mod</span><span class="o">.</span><span class="n">params</span><span class="p">[</span><span class="s1">&#39;age&#39;</span><span class="p">]</span>
<span class="n">intercept</span> <span class="o">=</span> <span class="n">fitted_log_reg_mod</span><span class="o">.</span><span class="n">params</span><span class="p">[</span><span class="s1">&#39;Intercept&#39;</span><span class="p">]</span>
</pre></div>
</div>
</div>
</div>
<p>We can now calculate the predicted log odds of survival, for each passenger.</p>
<p>(You’ll notice that this is done using the same formula as in linear regression):</p>
<p><span class="math notranslate nohighlight">\(\Large \vec{\hat{y}} = b_1 \vec{x_1} + b_2 \vec{x_2} + \text{c}\)</span></p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># calculate the predicted log odds of survival</span>
<span class="n">sample_df</span><span class="p">[</span><span class="s1">&#39;predicted_log_odds_of_survival&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">x1_slope</span><span class="o">*</span><span class="n">fare</span> <span class="o">+</span> <span class="n">x2_slope</span><span class="o">*</span><span class="n">age</span> <span class="o">+</span> <span class="n">intercept</span>
<span class="n">sample_df</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>age</th>
      <th>fare</th>
      <th>survived</th>
      <th>gender</th>
      <th>survived_dummy</th>
      <th>predicted_log_odds_of_survival</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>26.0</td>
      <td>7.1711</td>
      <td>no</td>
      <td>male</td>
      <td>0</td>
      <td>-2.871420</td>
    </tr>
    <tr>
      <th>1</th>
      <td>43.0</td>
      <td>26.0000</td>
      <td>yes</td>
      <td>female</td>
      <td>1</td>
      <td>1.722766</td>
    </tr>
    <tr>
      <th>2</th>
      <td>24.0</td>
      <td>8.0100</td>
      <td>no</td>
      <td>male</td>
      <td>0</td>
      <td>-2.970319</td>
    </tr>
    <tr>
      <th>3</th>
      <td>42.0</td>
      <td>7.1100</td>
      <td>no</td>
      <td>male</td>
      <td>0</td>
      <td>-1.118678</td>
    </tr>
    <tr>
      <th>4</th>
      <td>30.0</td>
      <td>13.0000</td>
      <td>no</td>
      <td>male</td>
      <td>0</td>
      <td>-1.588213</td>
    </tr>
    <tr>
      <th>5</th>
      <td>22.0</td>
      <td>7.1511</td>
      <td>no</td>
      <td>male</td>
      <td>0</td>
      <td>-3.314706</td>
    </tr>
    <tr>
      <th>6</th>
      <td>19.0</td>
      <td>8.0100</td>
      <td>no</td>
      <td>male</td>
      <td>0</td>
      <td>-3.520812</td>
    </tr>
    <tr>
      <th>7</th>
      <td>25.0</td>
      <td>13.0000</td>
      <td>no</td>
      <td>male</td>
      <td>0</td>
      <td>-2.138706</td>
    </tr>
    <tr>
      <th>8</th>
      <td>32.0</td>
      <td>15.1000</td>
      <td>no</td>
      <td>female</td>
      <td>0</td>
      <td>-1.064372</td>
    </tr>
    <tr>
      <th>9</th>
      <td>50.0</td>
      <td>65.0000</td>
      <td>yes</td>
      <td>female</td>
      <td>1</td>
      <td>8.132548</td>
    </tr>
    <tr>
      <th>10</th>
      <td>62.0</td>
      <td>10.1000</td>
      <td>yes</td>
      <td>male</td>
      <td>1</td>
      <td>1.515624</td>
    </tr>
    <tr>
      <th>11</th>
      <td>21.0</td>
      <td>7.1500</td>
      <td>no</td>
      <td>female</td>
      <td>0</td>
      <td>-3.424964</td>
    </tr>
    <tr>
      <th>12</th>
      <td>26.0</td>
      <td>7.1711</td>
      <td>no</td>
      <td>male</td>
      <td>0</td>
      <td>-2.871420</td>
    </tr>
    <tr>
      <th>13</th>
      <td>34.0</td>
      <td>81.1702</td>
      <td>yes</td>
      <td>female</td>
      <td>1</td>
      <td>8.709054</td>
    </tr>
    <tr>
      <th>14</th>
      <td>21.0</td>
      <td>31.1000</td>
      <td>no</td>
      <td>male</td>
      <td>0</td>
      <td>0.038017</td>
    </tr>
    <tr>
      <th>15</th>
      <td>52.0</td>
      <td>30.0000</td>
      <td>yes</td>
      <td>female</td>
      <td>1</td>
      <td>3.292022</td>
    </tr>
    <tr>
      <th>16</th>
      <td>19.0</td>
      <td>26.0508</td>
      <td>yes</td>
      <td>female</td>
      <td>1</td>
      <td>-0.912255</td>
    </tr>
    <tr>
      <th>17</th>
      <td>23.0</td>
      <td>13.0000</td>
      <td>no</td>
      <td>male</td>
      <td>0</td>
      <td>-2.358903</td>
    </tr>
    <tr>
      <th>18</th>
      <td>9.0</td>
      <td>46.1800</td>
      <td>no</td>
      <td>male</td>
      <td>0</td>
      <td>0.897283</td>
    </tr>
    <tr>
      <th>19</th>
      <td>22.0</td>
      <td>7.0406</td>
      <td>no</td>
      <td>male</td>
      <td>0</td>
      <td>-3.330684</td>
    </tr>
    <tr>
      <th>20</th>
      <td>36.0</td>
      <td>8.0100</td>
      <td>no</td>
      <td>male</td>
      <td>0</td>
      <td>-1.649136</td>
    </tr>
    <tr>
      <th>21</th>
      <td>36.0</td>
      <td>120.0000</td>
      <td>yes</td>
      <td>male</td>
      <td>1</td>
      <td>14.543734</td>
    </tr>
    <tr>
      <th>22</th>
      <td>5.0</td>
      <td>27.1800</td>
      <td>no</td>
      <td>male</td>
      <td>0</td>
      <td>-2.290361</td>
    </tr>
    <tr>
      <th>23</th>
      <td>17.0</td>
      <td>7.1711</td>
      <td>no</td>
      <td>male</td>
      <td>0</td>
      <td>-3.862307</td>
    </tr>
    <tr>
      <th>24</th>
      <td>1.0</td>
      <td>39.0000</td>
      <td>yes</td>
      <td>male</td>
      <td>1</td>
      <td>-1.021677</td>
    </tr>
    <tr>
      <th>25</th>
      <td>30.0</td>
      <td>13.1702</td>
      <td>yes</td>
      <td>female</td>
      <td>1</td>
      <td>-1.563603</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
<p>We can then plot these log odds predictions:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># generate the plot</span>
<span class="n">plotly_3D_with_plane</span><span class="p">(</span><span class="n">sample_df</span><span class="p">,</span> <span class="s1">&#39;fare&#39;</span><span class="p">,</span><span class="s1">&#39;age&#39;</span><span class="p">,</span> <span class="s1">&#39;predicted_log_odds_of_survival&#39;</span><span class="p">,</span> <span class="p">[</span><span class="s1">&#39;predicted_log_odds_of_survival&#39;</span><span class="p">],</span>
                         <span class="n">x1_slope</span><span class="p">,</span> <span class="n">x2_slope</span><span class="p">,</span> <span class="n">intercept</span><span class="p">,</span> <span class="n">y_1_or_0</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
</div>
</div>
<p>You can see that presently (on the scale of the log odds, rather than on the scale of the original data) everything looks <em>a lot</em> like linear regression.</p>
<p>This is not by accident - logistic regression is a generalized <strong>linear</strong> model (GLM). It is linear on some scale, in this case the log odds scale, and we use transformations (in this case the <code class="docutils literal notranslate"><span class="pre">inverse_logit()</span></code>) to transform between the linear scale and the scale of the data.</p>
<p>Here is the same model, but with the log odds converted to probabilities using <code class="docutils literal notranslate"><span class="pre">inverse_logit()</span></code>:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># plot the model with probabilities</span>
<span class="n">plotly_3D_with_plane</span><span class="p">(</span><span class="n">sample_df</span><span class="p">,</span> <span class="s1">&#39;fare&#39;</span><span class="p">,</span><span class="s1">&#39;age&#39;</span><span class="p">,</span> <span class="s1">&#39;survived_dummy&#39;</span><span class="p">,</span> <span class="p">[</span><span class="s1">&#39;survived&#39;</span><span class="p">],</span>
                         <span class="n">x1_slope</span><span class="p">,</span> <span class="n">x2_slope</span><span class="p">,</span> <span class="n">intercept</span><span class="p">,</span> <span class="n">probability</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
</div>
</div>
<p>The advantage of a generalized linear model is that it let’s us use the machinery of linear regression with different types of outcome variable.</p>
<ul class="simple">
<li><p>we can include categorical predictors in the same way as for linear regression</p></li>
<li><p>the “other predictors being equal” interpretation still applies to the slopes e.g. each slope now tells us the predicted change in the odds of survival for a 1-unit change in the predictor, holding all other predictors constant</p></li>
</ul>
</section>
<section id="model-evaluation-goodness-of-fit-and-model-comparison">
<h2>Model Evaluation (Goodness-of-Fit) and Model Comparison<a class="headerlink" href="#model-evaluation-goodness-of-fit-and-model-comparison" title="Link to this heading">#</a></h2>
<p>So far we’ve fit two logistic regression models:</p>
<p><code class="docutils literal notranslate"><span class="pre">survived</span> <span class="pre">~</span> <span class="pre">fare</span></code></p>
<p>and</p>
<p><code class="docutils literal notranslate"><span class="pre">survived</span> <span class="pre">~</span> <span class="pre">fare</span> <span class="pre">+</span> <span class="pre">age</span></code></p>
<p>But which model is better? Do we need the second predictor?</p>
<p>In order to answer this question we’ll need to do some model comparison, evaluation and selection.</p>
<p>This means we need a metric to assess the goodness-of-fit of each model. We can then compare the models
and select the model which is best.</p>
<p>There are several goodness-of-fit metrics we have seen so far (in linear regression and logistic regression):</p>
<ul class="simple">
<li><p>Sum of Squared error (lower is better)</p></li>
<li><p>Mean Squared Error (lower is better)</p></li>
<li><p>Root Mean Squared Error (lower is better)</p></li>
<li><p>(Log) Likelihood (higher is better)</p></li>
<li><p>Log loss (aka negative log likelihood) (lower is better)</p></li>
</ul>
<p>We can use these metrics to compare different models, and select a model that has a good balance of
goodness-of-fit to complexity (more on this later)!.</p>
<p><em>Accuracy</em> is a nice, intuitive measure to evaluate logistic regression models (or any model which
is fit to a binary categorical outcome variable); to which we will now turn.</p>
<section id="accuracy-for-models-with-a-categorical-outcome-variable">
<h3>Accuracy (for models with a categorical outcome variable)<a class="headerlink" href="#accuracy-for-models-with-a-categorical-outcome-variable" title="Link to this heading">#</a></h3>
<p>We can use the <code class="docutils literal notranslate"><span class="pre">predict()</span></code> method from a <code class="docutils literal notranslate"><span class="pre">statsmodels</span></code> fitted model, to generated predicted/fitted values from that model.</p>
<p>Let’s get the predictions from our two predictor model.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># show the two predictor model again</span>
<span class="n">fitted_log_reg_mod</span><span class="o">.</span><span class="n">summary</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><table class="simpletable">
<caption>Logit Regression Results</caption>
<tr>
  <th>Dep. Variable:</th>    <td>survived_dummy</td>  <th>  No. Observations:  </th>  <td>    26</td>  
</tr>
<tr>
  <th>Model:</th>                 <td>Logit</td>      <th>  Df Residuals:      </th>  <td>    23</td>  
</tr>
<tr>
  <th>Method:</th>                 <td>MLE</td>       <th>  Df Model:          </th>  <td>     2</td>  
</tr>
<tr>
  <th>Date:</th>            <td>Wed, 28 Jan 2026</td> <th>  Pseudo R-squ.:     </th>  <td>0.5089</td>  
</tr>
<tr>
  <th>Time:</th>                <td>11:43:35</td>     <th>  Log-Likelihood:    </th> <td> -8.2360</td> 
</tr>
<tr>
  <th>converged:</th>             <td>True</td>       <th>  LL-Null:           </th> <td> -16.771</td> 
</tr>
<tr>
  <th>Covariance Type:</th>     <td>nonrobust</td>    <th>  LLR p-value:       </th> <td>0.0001965</td>
</tr>
</table>
<table class="simpletable">
<tr>
      <td></td>         <th>coef</th>     <th>std err</th>      <th>z</th>      <th>P>|z|</th>  <th>[0.025</th>    <th>0.975]</th>  
</tr>
<tr>
  <th>Intercept</th> <td>   -6.7709</td> <td>    2.602</td> <td>   -2.602</td> <td> 0.009</td> <td>  -11.871</td> <td>   -1.671</td>
</tr>
<tr>
  <th>fare</th>      <td>    0.1446</td> <td>    0.065</td> <td>    2.223</td> <td> 0.026</td> <td>    0.017</td> <td>    0.272</td>
</tr>
<tr>
  <th>age</th>       <td>    0.1101</td> <td>    0.055</td> <td>    2.009</td> <td> 0.045</td> <td>    0.003</td> <td>    0.218</td>
</tr>
</table></div></div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># generate the predictions</span>
<span class="n">fitted_log_reg_mod</span><span class="o">.</span><span class="n">predict</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>array([0.05358458, 0.84848476, 0.04878491, 0.24625666, 0.16963546,
       0.0350701 , 0.02872583, 0.10539134, 0.25647476, 0.99970627,
       0.81989325, 0.03152432, 0.05358458, 0.99983494, 0.50950309,
       0.96415409, 0.28653868, 0.08636071, 0.71039084, 0.03453342,
       0.16122573, 0.99999952, 0.09192438, 0.02058672, 0.26470087,
       0.17313018])
</pre></div>
</div>
</div>
</div>
<p>The output contains the predicted probability of survival, for each observation in the data set:</p>
<p>In order to compute the accuracy goodness-of-fit metric, we need to convert
these probability predictions into actual category labels.</p>
<p>We do this by setting a “cutoff” at 0.5 - e.g.:</p>
<ul class="simple">
<li><p>if the predicted probability is over 0.5, we treat the prediction as being <code class="docutils literal notranslate"><span class="pre">1</span></code> (e.g. <code class="docutils literal notranslate"><span class="pre">survived</span></code>)</p></li>
<li><p>if the predicted probability is below 0.5 we treat the prediction as being <code class="docutils literal notranslate"><span class="pre">0</span></code> (e.g. <code class="docutils literal notranslate"><span class="pre">died</span></code>).</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># convert the predicted probabilities to outcome category dummy codes</span>
<span class="n">predictions</span> <span class="o">=</span> <span class="n">fitted_log_reg_mod</span><span class="o">.</span><span class="n">predict</span><span class="p">()</span> <span class="o">&gt;</span> <span class="mf">0.5</span>
<span class="n">predictions</span> <span class="o">=</span> <span class="n">predictions</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="s1">&#39;int&#39;</span><span class="p">)</span>
<span class="n">predictions</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>array([0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1,
       0, 0, 0, 0])
</pre></div>
</div>
</div>
</div>
<p>Let’s put the actual <code class="docutils literal notranslate"><span class="pre">survived_dummy</span></code> scores alongside the predicted <code class="docutils literal notranslate"><span class="pre">survived_dummy</span></code> scores
from the two predictor model:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># a dataframe containing the actual scores and the predicted scores</span>
<span class="n">eval_df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">({</span><span class="s1">&#39;survived_dummy&#39;</span><span class="p">:</span> <span class="n">sample_df</span><span class="p">[</span><span class="s1">&#39;survived_dummy&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">values</span><span class="p">,</span>
                       <span class="s1">&#39;predictions&#39;</span><span class="p">:</span> <span class="n">predictions</span><span class="p">})</span>

<span class="n">eval_df</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>survived_dummy</th>
      <th>predictions</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>1</th>
      <td>1</td>
      <td>1</td>
    </tr>
    <tr>
      <th>2</th>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>3</th>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>4</th>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>5</th>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>6</th>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>7</th>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>8</th>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>9</th>
      <td>1</td>
      <td>1</td>
    </tr>
    <tr>
      <th>10</th>
      <td>1</td>
      <td>1</td>
    </tr>
    <tr>
      <th>11</th>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>12</th>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>13</th>
      <td>1</td>
      <td>1</td>
    </tr>
    <tr>
      <th>14</th>
      <td>0</td>
      <td>1</td>
    </tr>
    <tr>
      <th>15</th>
      <td>1</td>
      <td>1</td>
    </tr>
    <tr>
      <th>16</th>
      <td>1</td>
      <td>0</td>
    </tr>
    <tr>
      <th>17</th>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>18</th>
      <td>0</td>
      <td>1</td>
    </tr>
    <tr>
      <th>19</th>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>20</th>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>21</th>
      <td>1</td>
      <td>1</td>
    </tr>
    <tr>
      <th>22</th>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>23</th>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>24</th>
      <td>1</td>
      <td>0</td>
    </tr>
    <tr>
      <th>25</th>
      <td>1</td>
      <td>0</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
<p>We can now use the <code class="docutils literal notranslate"><span class="pre">==</span></code> comparison operation to add a new column, showing whether our model correctly
predicted each passenger’s <code class="docutils literal notranslate"><span class="pre">survived_dummy</span></code> score:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># add a column showing if the prediction was correct</span>
<span class="n">eval_df</span><span class="p">[</span><span class="s1">&#39;Correct&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span><span class="n">eval_df</span><span class="p">[</span><span class="s1">&#39;survived_dummy&#39;</span><span class="p">]</span> <span class="o">==</span> <span class="n">eval_df</span><span class="p">[</span><span class="s1">&#39;predictions&#39;</span><span class="p">])</span>
<span class="n">eval_df</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>survived_dummy</th>
      <th>predictions</th>
      <th>Correct</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>0</td>
      <td>0</td>
      <td>True</td>
    </tr>
    <tr>
      <th>1</th>
      <td>1</td>
      <td>1</td>
      <td>True</td>
    </tr>
    <tr>
      <th>2</th>
      <td>0</td>
      <td>0</td>
      <td>True</td>
    </tr>
    <tr>
      <th>3</th>
      <td>0</td>
      <td>0</td>
      <td>True</td>
    </tr>
    <tr>
      <th>4</th>
      <td>0</td>
      <td>0</td>
      <td>True</td>
    </tr>
    <tr>
      <th>5</th>
      <td>0</td>
      <td>0</td>
      <td>True</td>
    </tr>
    <tr>
      <th>6</th>
      <td>0</td>
      <td>0</td>
      <td>True</td>
    </tr>
    <tr>
      <th>7</th>
      <td>0</td>
      <td>0</td>
      <td>True</td>
    </tr>
    <tr>
      <th>8</th>
      <td>0</td>
      <td>0</td>
      <td>True</td>
    </tr>
    <tr>
      <th>9</th>
      <td>1</td>
      <td>1</td>
      <td>True</td>
    </tr>
    <tr>
      <th>10</th>
      <td>1</td>
      <td>1</td>
      <td>True</td>
    </tr>
    <tr>
      <th>11</th>
      <td>0</td>
      <td>0</td>
      <td>True</td>
    </tr>
    <tr>
      <th>12</th>
      <td>0</td>
      <td>0</td>
      <td>True</td>
    </tr>
    <tr>
      <th>13</th>
      <td>1</td>
      <td>1</td>
      <td>True</td>
    </tr>
    <tr>
      <th>14</th>
      <td>0</td>
      <td>1</td>
      <td>False</td>
    </tr>
    <tr>
      <th>15</th>
      <td>1</td>
      <td>1</td>
      <td>True</td>
    </tr>
    <tr>
      <th>16</th>
      <td>1</td>
      <td>0</td>
      <td>False</td>
    </tr>
    <tr>
      <th>17</th>
      <td>0</td>
      <td>0</td>
      <td>True</td>
    </tr>
    <tr>
      <th>18</th>
      <td>0</td>
      <td>1</td>
      <td>False</td>
    </tr>
    <tr>
      <th>19</th>
      <td>0</td>
      <td>0</td>
      <td>True</td>
    </tr>
    <tr>
      <th>20</th>
      <td>0</td>
      <td>0</td>
      <td>True</td>
    </tr>
    <tr>
      <th>21</th>
      <td>1</td>
      <td>1</td>
      <td>True</td>
    </tr>
    <tr>
      <th>22</th>
      <td>0</td>
      <td>0</td>
      <td>True</td>
    </tr>
    <tr>
      <th>23</th>
      <td>0</td>
      <td>0</td>
      <td>True</td>
    </tr>
    <tr>
      <th>24</th>
      <td>1</td>
      <td>0</td>
      <td>False</td>
    </tr>
    <tr>
      <th>25</th>
      <td>1</td>
      <td>0</td>
      <td>False</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
<p>We can then calculate the accuracy goodness-of-fit metric using:</p>
<p><span class="math notranslate nohighlight">\(\Large \text{accuracy} = \frac{\text{correct predictions}}{\text{number of predictions}} \)</span></p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># calculate the accuracy</span>
<span class="nb">sum</span><span class="p">(</span><span class="n">eval_df</span><span class="p">[</span><span class="s1">&#39;Correct&#39;</span><span class="p">])</span><span class="o">/</span><span class="nb">len</span><span class="p">(</span><span class="n">eval_df</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>0.8076923076923077
</pre></div>
</div>
</div>
</div>
<p>Taking the mean of a column of Boolean values gives us the same proportion, so as a shortcut we can use:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">eval_df</span><span class="p">[</span><span class="s1">&#39;Correct&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>np.float64(0.8076923076923077)
</pre></div>
</div>
</div>
</div>
<p>The <a class="reference external" href="https://scikit-learn.org/stable/"><code class="docutils literal notranslate"><span class="pre">scikit-learn</span></code></a> library has a many helpful functions for model comparison and selection.</p>
<p>The <code class="docutils literal notranslate"><span class="pre">confusion_matrix()</span></code> function is very useful for evaluating models (like logistic regression) that predict binary-categorical variables:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.metrics</span><span class="w"> </span><span class="kn">import</span> <span class="n">confusion_matrix</span>
<span class="n">confusion_matrix_2_predictors</span> <span class="o">=</span> <span class="n">confusion_matrix</span><span class="p">(</span><span class="n">sample_df</span><span class="p">[</span><span class="s1">&#39;survived_dummy&#39;</span><span class="p">],</span> <span class="n">predictions</span><span class="p">)</span>
<span class="n">confusion_matrix_2_predictors</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>array([[15,  2],
       [ 3,  6]])
</pre></div>
</div>
</div>
</div>
<p>We’re sure it’s clear as mud what the meaning of that output is!</p>
<p><code class="docutils literal notranslate"><span class="pre">scikit-learn</span></code> functions and models are generally less “user-friendly” and informative (or verbose, depending on your opinion) that <code class="docutils literal notranslate"><span class="pre">statsmodels</span></code> functions/models.</p>
<p>Fortunately, we can make the meaning of the confusion matrix clearer, using the <code class="docutils literal notranslate"><span class="pre">ConfusionMatrixDisplay()</span></code> function:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># make a clearer display of the confusion matrix</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.metrics</span><span class="w"> </span><span class="kn">import</span> <span class="n">ConfusionMatrixDisplay</span>

<span class="n">ConfusionMatrixDisplay</span><span class="p">(</span><span class="n">confusion_matrix_2_predictors</span><span class="p">)</span><span class="o">.</span><span class="n">plot</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;Predicted Category</span><span class="se">\n</span><span class="s1">{1 == Yes; 0 == No}&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;Survived Dummy</span><span class="se">\n</span><span class="s1">{1 == Yes; 0 == No}&#39;</span><span class="p">);</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/b0cad03756724e6e9e66745b425194dc3f878a82c4556a693c9c48db49100769.png" src="_images/b0cad03756724e6e9e66745b425194dc3f878a82c4556a693c9c48db49100769.png" />
</div>
</div>
<p>The confusion matrix summarizes how good the models predictions were.</p>
<p>The top left cell shows the number of <em>true negative</em> predictions - the passenger did <em>not</em> survive and the model predicted they did not.
The top right cell shows the number of <em>false positive</em> predictions - the passenger did <em>not</em> survive, but the model predicted they did.
The bottom left cell shows the number of <em>false negative</em> predictions - the passenger survived, but the model predicted they did not.
The bottom right cell shows the number of <em>true positive</em> predictions - the passenger survived, and the model correctly predicted they did.</p>
<p>(Thinking about these can be a bit brain-bendy at first, so take a few moments to make sure you understand the matrix).</p>
<p>The confusion matrix can be indexed like a normal array, so let’s pull out the true positive count, false positive count, true negative count and false negative count as separate variables:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># get the counts</span>
<span class="n">true_negative</span> <span class="o">=</span> <span class="n">confusion_matrix_2_predictors</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">]</span>
<span class="n">false_positive</span> <span class="o">=</span> <span class="n">confusion_matrix_2_predictors</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span>
<span class="n">true_positive</span> <span class="o">=</span> <span class="n">confusion_matrix_2_predictors</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span>
<span class="n">false_negative</span> <span class="o">=</span> <span class="n">confusion_matrix_2_predictors</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">]</span>

<span class="c1"># get the total number of observations</span>
<span class="n">total_n</span> <span class="o">=</span> <span class="n">true_positive</span> <span class="o">+</span> <span class="n">true_negative</span> <span class="o">+</span> <span class="n">false_positive</span> <span class="o">+</span> <span class="n">false_negative</span>

<span class="c1"># show the accuracy, computed from the confusion matrix</span>
<span class="n">accuracy</span> <span class="o">=</span> <span class="p">(</span><span class="n">true_negative</span> <span class="o">+</span> <span class="n">true_positive</span><span class="p">)</span><span class="o">/</span><span class="n">total_n</span>

<span class="n">accuracy</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>np.float64(0.8076923076923077)
</pre></div>
</div>
</div>
</div>
<p>Let’s compare the accuracy of the two prediction model, to that of the single predictor model:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># remind ourselves of the single predictor model</span>
<span class="n">fitted_log_reg_mod_single_predictor</span><span class="o">.</span><span class="n">summary</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><table class="simpletable">
<caption>Logit Regression Results</caption>
<tr>
  <th>Dep. Variable:</th>    <td>survived_dummy</td>  <th>  No. Observations:  </th>  <td>    26</td> 
</tr>
<tr>
  <th>Model:</th>                 <td>Logit</td>      <th>  Df Residuals:      </th>  <td>    24</td> 
</tr>
<tr>
  <th>Method:</th>                 <td>MLE</td>       <th>  Df Model:          </th>  <td>     1</td> 
</tr>
<tr>
  <th>Date:</th>            <td>Wed, 28 Jan 2026</td> <th>  Pseudo R-squ.:     </th>  <td>0.3104</td> 
</tr>
<tr>
  <th>Time:</th>                <td>11:43:35</td>     <th>  Log-Likelihood:    </th> <td> -11.565</td>
</tr>
<tr>
  <th>converged:</th>             <td>True</td>       <th>  LL-Null:           </th> <td> -16.771</td>
</tr>
<tr>
  <th>Covariance Type:</th>     <td>nonrobust</td>    <th>  LLR p-value:       </th> <td>0.001252</td>
</tr>
</table>
<table class="simpletable">
<tr>
      <td></td>         <th>coef</th>     <th>std err</th>      <th>z</th>      <th>P>|z|</th>  <th>[0.025</th>    <th>0.975]</th>  
</tr>
<tr>
  <th>Intercept</th> <td>   -2.5415</td> <td>    0.944</td> <td>   -2.691</td> <td> 0.007</td> <td>   -4.392</td> <td>   -0.691</td>
</tr>
<tr>
  <th>fare</th>      <td>    0.0828</td> <td>    0.039</td> <td>    2.146</td> <td> 0.032</td> <td>    0.007</td> <td>    0.158</td>
</tr>
</table></div></div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># generated `survived` predictions for the single predictor model</span>
<span class="n">predictions_single_predictor</span> <span class="o">=</span> <span class="n">fitted_log_reg_mod_single_predictor</span><span class="o">.</span><span class="n">predict</span><span class="p">()</span> <span class="o">&gt;</span> <span class="mf">0.5</span>
<span class="n">predictions_single_predictor</span> <span class="o">=</span> <span class="n">predictions_single_predictor</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="s1">&#39;int&#39;</span><span class="p">)</span>
<span class="n">predictions_single_predictor</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>array([0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1,
       0, 0, 1, 0])
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># generate a confusion matrix from the single predictor model</span>
<span class="n">confusion_matrix_1_predictor</span> <span class="o">=</span> <span class="n">confusion_matrix</span><span class="p">(</span><span class="n">sample_df</span><span class="p">[</span><span class="s1">&#39;survived_dummy&#39;</span><span class="p">],</span> <span class="n">predictions_single_predictor</span><span class="p">)</span>
<span class="n">ConfusionMatrixDisplay</span><span class="p">(</span><span class="n">confusion_matrix_1_predictor</span><span class="p">)</span><span class="o">.</span><span class="n">plot</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;Predicted Category</span><span class="se">\n</span><span class="s1">{1 == Yes; 0 == No}&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;Survived Dummy</span><span class="se">\n</span><span class="s1">{1 == Yes; 0 == No}&#39;</span><span class="p">);</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/9510bc11ca1baf8a9623105c2982cbcfa4583305ab4c991ffc4cbd054b437990.png" src="_images/9510bc11ca1baf8a9623105c2982cbcfa4583305ab4c991ffc4cbd054b437990.png" />
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># calculate the accuracy from the single predictor model</span>
<span class="p">(</span><span class="n">confusion_matrix_1_predictor</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">]</span> <span class="o">+</span> <span class="n">confusion_matrix_1_predictor</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">])</span><span class="o">/</span><span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">confusion_matrix_1_predictor</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>np.float64(0.7307692307692307)
</pre></div>
</div>
</div>
</div>
</section>
</section>
<section id="test-train-splits">
<h2>Test/Train splits<a class="headerlink" href="#test-train-splits" title="Link to this heading">#</a></h2>
<p>This accuracy (and other goodness-of-fit metrics) give us a way of comparing between models, like the
two models we have compared here.</p>
<p>We would like our model to generate good predictions. However, typically we are interested in <em>populations</em>, and we must use <em>samples</em> to make
<em>inferences</em> about those populations.</p>
<p>Another way of saying this is that we would like our model to generalize well when making predictions about unseen data from the same population.</p>
<p>It might be tempting to think that more predictors included in the model equals better accuracy on the sample data equals better accuracy on unseen data.</p>
<p>However, it’s typically not a good idea to include too many predictors in a model. If a model is very <em>complex</em> (e.g. includes many predictors) it can lead to <em>overfitting</em>. Overfitting is where the model fits to noise in our particular sample which is not representative of the population from which the data came. This entails that the model will perform badly when making predictions about unseen data.</p>
<p>More often, simpler models will generalize better.</p>
<p>To avoid overfitting, we can use a test/train split. This involves splitting the data (for instance, we might use 80% of the data as a training set and 20% as a test set). We train/fit the model to the training data and then evaluate it against the test data. This allows us to simulate the process of evaluating the model against unseen data from the same population:</p>
<p><img alt="" src="_images/test-train-split.png" /></p>
<p>We then aim, through model comparison, to find the model that has the best “sweet spot” between complexity, and performance when predicting the unseen values in the test set:</p>
<p><img alt="" src="_images/overfitting-underfitting.png" /></p>
<p>We can create a test/train split manually, using indexing:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">sample_df_train</span> <span class="o">=</span> <span class="n">sample_df</span><span class="o">.</span><span class="n">iloc</span><span class="p">[:</span><span class="mi">20</span><span class="p">]</span>
<span class="n">sample_df_test</span> <span class="o">=</span> <span class="n">sample_df</span><span class="o">.</span><span class="n">iloc</span><span class="p">[</span><span class="mi">20</span><span class="p">:]</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># show the training set</span>
<span class="n">sample_df_train</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>age</th>
      <th>fare</th>
      <th>survived</th>
      <th>gender</th>
      <th>survived_dummy</th>
      <th>predicted_log_odds_of_survival</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>26.0</td>
      <td>7.1711</td>
      <td>no</td>
      <td>male</td>
      <td>0</td>
      <td>-2.871420</td>
    </tr>
    <tr>
      <th>1</th>
      <td>43.0</td>
      <td>26.0000</td>
      <td>yes</td>
      <td>female</td>
      <td>1</td>
      <td>1.722766</td>
    </tr>
    <tr>
      <th>2</th>
      <td>24.0</td>
      <td>8.0100</td>
      <td>no</td>
      <td>male</td>
      <td>0</td>
      <td>-2.970319</td>
    </tr>
    <tr>
      <th>3</th>
      <td>42.0</td>
      <td>7.1100</td>
      <td>no</td>
      <td>male</td>
      <td>0</td>
      <td>-1.118678</td>
    </tr>
    <tr>
      <th>4</th>
      <td>30.0</td>
      <td>13.0000</td>
      <td>no</td>
      <td>male</td>
      <td>0</td>
      <td>-1.588213</td>
    </tr>
    <tr>
      <th>5</th>
      <td>22.0</td>
      <td>7.1511</td>
      <td>no</td>
      <td>male</td>
      <td>0</td>
      <td>-3.314706</td>
    </tr>
    <tr>
      <th>6</th>
      <td>19.0</td>
      <td>8.0100</td>
      <td>no</td>
      <td>male</td>
      <td>0</td>
      <td>-3.520812</td>
    </tr>
    <tr>
      <th>7</th>
      <td>25.0</td>
      <td>13.0000</td>
      <td>no</td>
      <td>male</td>
      <td>0</td>
      <td>-2.138706</td>
    </tr>
    <tr>
      <th>8</th>
      <td>32.0</td>
      <td>15.1000</td>
      <td>no</td>
      <td>female</td>
      <td>0</td>
      <td>-1.064372</td>
    </tr>
    <tr>
      <th>9</th>
      <td>50.0</td>
      <td>65.0000</td>
      <td>yes</td>
      <td>female</td>
      <td>1</td>
      <td>8.132548</td>
    </tr>
    <tr>
      <th>10</th>
      <td>62.0</td>
      <td>10.1000</td>
      <td>yes</td>
      <td>male</td>
      <td>1</td>
      <td>1.515624</td>
    </tr>
    <tr>
      <th>11</th>
      <td>21.0</td>
      <td>7.1500</td>
      <td>no</td>
      <td>female</td>
      <td>0</td>
      <td>-3.424964</td>
    </tr>
    <tr>
      <th>12</th>
      <td>26.0</td>
      <td>7.1711</td>
      <td>no</td>
      <td>male</td>
      <td>0</td>
      <td>-2.871420</td>
    </tr>
    <tr>
      <th>13</th>
      <td>34.0</td>
      <td>81.1702</td>
      <td>yes</td>
      <td>female</td>
      <td>1</td>
      <td>8.709054</td>
    </tr>
    <tr>
      <th>14</th>
      <td>21.0</td>
      <td>31.1000</td>
      <td>no</td>
      <td>male</td>
      <td>0</td>
      <td>0.038017</td>
    </tr>
    <tr>
      <th>15</th>
      <td>52.0</td>
      <td>30.0000</td>
      <td>yes</td>
      <td>female</td>
      <td>1</td>
      <td>3.292022</td>
    </tr>
    <tr>
      <th>16</th>
      <td>19.0</td>
      <td>26.0508</td>
      <td>yes</td>
      <td>female</td>
      <td>1</td>
      <td>-0.912255</td>
    </tr>
    <tr>
      <th>17</th>
      <td>23.0</td>
      <td>13.0000</td>
      <td>no</td>
      <td>male</td>
      <td>0</td>
      <td>-2.358903</td>
    </tr>
    <tr>
      <th>18</th>
      <td>9.0</td>
      <td>46.1800</td>
      <td>no</td>
      <td>male</td>
      <td>0</td>
      <td>0.897283</td>
    </tr>
    <tr>
      <th>19</th>
      <td>22.0</td>
      <td>7.0406</td>
      <td>no</td>
      <td>male</td>
      <td>0</td>
      <td>-3.330684</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># show the test set</span>
<span class="n">sample_df_test</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>age</th>
      <th>fare</th>
      <th>survived</th>
      <th>gender</th>
      <th>survived_dummy</th>
      <th>predicted_log_odds_of_survival</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>20</th>
      <td>36.0</td>
      <td>8.0100</td>
      <td>no</td>
      <td>male</td>
      <td>0</td>
      <td>-1.649136</td>
    </tr>
    <tr>
      <th>21</th>
      <td>36.0</td>
      <td>120.0000</td>
      <td>yes</td>
      <td>male</td>
      <td>1</td>
      <td>14.543734</td>
    </tr>
    <tr>
      <th>22</th>
      <td>5.0</td>
      <td>27.1800</td>
      <td>no</td>
      <td>male</td>
      <td>0</td>
      <td>-2.290361</td>
    </tr>
    <tr>
      <th>23</th>
      <td>17.0</td>
      <td>7.1711</td>
      <td>no</td>
      <td>male</td>
      <td>0</td>
      <td>-3.862307</td>
    </tr>
    <tr>
      <th>24</th>
      <td>1.0</td>
      <td>39.0000</td>
      <td>yes</td>
      <td>male</td>
      <td>1</td>
      <td>-1.021677</td>
    </tr>
    <tr>
      <th>25</th>
      <td>30.0</td>
      <td>13.1702</td>
      <td>yes</td>
      <td>female</td>
      <td>1</td>
      <td>-1.563603</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
<p>However, <code class="docutils literal notranslate"><span class="pre">scikit-learn</span></code> contains useful functions that will create the split also:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.model_selection</span><span class="w"> </span><span class="kn">import</span> <span class="n">train_test_split</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.linear_model</span><span class="w"> </span><span class="kn">import</span> <span class="n">LogisticRegression</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.metrics</span><span class="w"> </span><span class="kn">import</span> <span class="n">accuracy_score</span>

<span class="c1"># separate out our predictors (X) from our outcome variable (Y)</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">sample_df</span><span class="p">[[</span><span class="s1">&#39;fare&#39;</span><span class="p">,</span> <span class="s1">&#39;age&#39;</span><span class="p">]]</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">sample_df</span><span class="p">[</span><span class="s1">&#39;survived_dummy&#39;</span><span class="p">]</span>

<span class="c1"># create the test/train split</span>
<span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span>
                                                    <span class="n">train_size</span><span class="o">=</span><span class="mf">0.7</span><span class="p">)</span>

<span class="n">jupyprint</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Length of X_train: </span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="n">X_train</span><span class="p">)</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="n">jupyprint</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Length of X_test: </span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="n">jupyprint</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Length of y_train: </span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="n">y_train</span><span class="p">)</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="n">jupyprint</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Length of y_test: </span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="n">y_test</span><span class="p">)</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

<span class="c1"># fit a logistic regression (using sklearn)</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">LogisticRegression</span><span class="p">()</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span>  <span class="n">y_train</span><span class="p">)</span>

<span class="c1"># evaluate the model on the test set, and show the accuracy</span>
<span class="n">y_test_predictions</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>
<span class="n">jupyprint</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Accuracy on testing set: </span><span class="si">{</span><span class="n">accuracy_score</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span><span class="w"> </span><span class="n">y_test_predictions</span><span class="p">)</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<p>Length of X_train: 18</p>
<p>Length of X_test: 8</p>
<p>Length of y_train: 18</p>
<p>Length of y_test: 8</p>
<p>Accuracy on testing set: 0.875</p>
</div>
</div>
<p>We can then repeat this process using the simpler model, and compare each models accuracy on the test data:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># modify the test data to include only one predictor</span>
<span class="n">X_train_2</span> <span class="o">=</span> <span class="n">X_train</span><span class="p">[</span><span class="s1">&#39;fare&#39;</span><span class="p">]</span>
<span class="n">X_test_2</span> <span class="o">=</span> <span class="n">X_test</span><span class="p">[</span><span class="s1">&#39;fare&#39;</span><span class="p">]</span>

<span class="c1"># fit a logistic regression to the training data</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">LogisticRegression</span><span class="p">()</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train_2</span><span class="o">.</span><span class="n">values</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span>  <span class="n">y_train</span><span class="p">)</span>

<span class="c1"># evaluate the perform against the test data</span>
<span class="n">y_test_predictions</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test_2</span><span class="o">.</span><span class="n">values</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
<span class="n">accuracy_score</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">y_test_predictions</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>0.875
</pre></div>
</div>
</div>
</div>
<p>Through using test/train splits, we can be more confident of the models performance with “unseen” data from the population - as we have simulated the process of testing it against it.</p>
<p>On the basis of this test/train split, it appears that adding much above the single predictor.</p>
</section>
<section id="cross-validation">
<h2>Cross-validation<a class="headerlink" href="#cross-validation" title="Link to this heading">#</a></h2>
<p>However, a disadvantage of the test/train split is that we have lost a portion of our data when fitting the model.</p>
<p>This can be an issue, especially where our sample is small. A better approach is to use <em>cross-validation</em>.</p>
<p>This is where we use a variety of different test/train splits on the same dataset.  We fit our model to the multiple different test/train splits - where each subset of the data is used both as a training set and as a test (validation) set.</p>
<p>Here is an illustration, each block represents the whole sample, the white part is the training set, and the blue part is the test set.</p>
<p><img alt="" src="_images/cross-validation.png" /></p>
<p>Through fitting a model to each “fold” of the cross-validation (e.g. fitting the model to the training component, and then evaluating it against the test component, on each trial) we get a better estimate of how well the model will generalize to unseen data.</p>
<p>The code cell below performs the cross-validation:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.model_selection</span><span class="w"> </span><span class="kn">import</span> <span class="n">cross_val_score</span>

<span class="c1"># perform a cross validation for the two predictor model</span>
<span class="n">survived_fare_age_cross_val</span> <span class="o">=</span> <span class="n">cross_val_score</span><span class="p">(</span><span class="n">LogisticRegression</span><span class="p">(),</span>
                                        <span class="n">sample_df</span><span class="p">[[</span><span class="s1">&#39;fare&#39;</span><span class="p">,</span> <span class="s1">&#39;age&#39;</span><span class="p">]],</span>
                                        <span class="n">sample_df</span><span class="p">[</span><span class="s1">&#39;survived_dummy&#39;</span><span class="p">],</span>
                                        <span class="n">cv</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span>
                                      <span class="n">scoring</span><span class="o">=</span><span class="s1">&#39;accuracy&#39;</span><span class="p">)</span>
<span class="c1"># show the cross validation result</span>
<span class="n">survived_fare_age_cross_val</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>array([1. , 1. , 0.8, 0.6, 0.6])
</pre></div>
</div>
</div>
</div>
<p>Each element in the array shows the accuracy score for the two predictor model fit to a specific test/train split.</p>
<p>We can take the mean accuracy to get a good estimate of well the model would predict unseen data.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">survived_fare_age_cross_val</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>np.float64(0.8)
</pre></div>
</div>
</div>
</div>
<p>We can then compare this to the single predictor model:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># cross-validation using the single predictor model</span>
<span class="n">survived_fare_cross_val</span> <span class="o">=</span> <span class="n">cross_val_score</span><span class="p">(</span><span class="n">LogisticRegression</span><span class="p">(),</span>
                                       <span class="n">sample_df</span><span class="p">[</span><span class="s1">&#39;fare&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">values</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span>
                                       <span class="n">sample_df</span><span class="p">[</span><span class="s1">&#39;survived_dummy&#39;</span><span class="p">],</span>
                                       <span class="n">cv</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span>
                                       <span class="n">scoring</span><span class="o">=</span><span class="s1">&#39;accuracy&#39;</span><span class="p">)</span>
<span class="n">survived_fare_cross_val</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>array([0.83333333, 0.8       , 0.6       , 0.8       , 0.8       ])
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">survived_fare_cross_val</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>np.float64(0.7666666666666666)
</pre></div>
</div>
</div>
</div>
<p>The two predictor model has better accuracy over the different test/train splits.</p>
<p>Let’s set up a loop to compare the accuracy of various models:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># add a dummy for gender</span>
<span class="n">df</span> <span class="o">=</span> <span class="n">sample_df</span>
<span class="n">df</span><span class="p">[</span><span class="s1">&#39;gender_dummy&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span><span class="n">df</span><span class="p">[</span><span class="s1">&#39;gender&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">replace</span><span class="p">([</span><span class="s1">&#39;male&#39;</span><span class="p">,</span> <span class="s1">&#39;female&#39;</span><span class="p">],</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">])</span>
                      <span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="nb">int</span><span class="p">))</span>

<span class="c1"># different combinations of predictors</span>
<span class="n">model_1_predictors</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;fare&#39;</span><span class="p">]</span>
<span class="n">model_2_predictors</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;fare&#39;</span><span class="p">,</span> <span class="s1">&#39;age&#39;</span><span class="p">]</span>
<span class="n">model_3_predictors</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;fare&#39;</span><span class="p">,</span> <span class="s1">&#39;age&#39;</span><span class="p">,</span> <span class="s1">&#39;gender_dummy&#39;</span><span class="p">]</span>

<span class="n">model_list</span> <span class="o">=</span> <span class="p">[</span><span class="n">model_1_predictors</span><span class="p">,</span> <span class="n">model_2_predictors</span><span class="p">,</span> <span class="n">model_3_predictors</span> <span class="p">]</span>

<span class="c1"># separate out predictors from the outcome variable</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">df</span><span class="p">[[</span><span class="s1">&#39;fare&#39;</span><span class="p">,</span> <span class="s1">&#39;age&#39;</span><span class="p">,</span> <span class="s1">&#39;gender_dummy&#39;</span><span class="p">]]</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="s1">&#39;survived_dummy&#39;</span><span class="p">]</span>

<span class="c1"># loop over the different combinations of predictors, and get cross-validated </span>
<span class="c1"># accuracy scores for them</span>
<span class="n">results_df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">()</span>
<span class="k">for</span> <span class="n">model</span> <span class="ow">in</span> <span class="n">model_list</span><span class="p">:</span>
    <span class="n">X_current_model</span> <span class="o">=</span> <span class="n">X</span><span class="p">[</span><span class="n">model</span><span class="p">]</span><span class="o">.</span><span class="n">values</span>

    <span class="n">jupyprint</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;==== Model: survived ~ </span><span class="si">{</span><span class="s2">&quot; + &quot;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">model</span><span class="p">)</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>

    <span class="n">current_model_cross_val_score</span> <span class="o">=</span> <span class="n">cross_val_score</span><span class="p">(</span><span class="n">LogisticRegression</span><span class="p">(),</span>
                                                      <span class="n">X_current_model</span><span class="p">,</span>
                                                      <span class="n">y</span><span class="p">,</span>
                                                      <span class="n">cv</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span>
                                                      <span class="n">scoring</span><span class="o">=</span><span class="s1">&#39;accuracy&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>

    <span class="n">jupyprint</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;Accuracy = </span><span class="si">{</span><span class="n">current_model_cross_val_score</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>
    <span class="n">results_df</span><span class="p">[</span><span class="s1">&#39; + &#39;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">model</span><span class="p">)]</span> <span class="o">=</span> <span class="p">[</span><span class="n">current_model_cross_val_score</span><span class="p">]</span>

<span class="c1"># show the results</span>
<span class="n">results_df</span><span class="o">.</span><span class="n">T</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<p>==== Model: survived ~ fare</p>
<p>Accuracy = 0.7666666666666666</p>
<p>==== Model: survived ~ fare + age</p>
<p>Accuracy = 0.8</p>
<p>==== Model: survived ~ fare + age + gender_dummy</p>
<p>Accuracy = 0.76</p>
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>0</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>fare</th>
      <td>0.766667</td>
    </tr>
    <tr>
      <th>fare + age</th>
      <td>0.800000</td>
    </tr>
    <tr>
      <th>fare + age + gender_dummy</th>
      <td>0.760000</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
<p>Because of the cross-validation, we can be more confident that each models mean accuracy score reflects how it will
generalize to unseen data.</p>
<p>We can also use this procedure for different <em>types</em> of model.</p>
<p>The cell below allows for comparison between the logistic regression model, and a naive Bayes classifier:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.naive_bayes</span><span class="w"> </span><span class="kn">import</span> <span class="n">GaussianNB</span>

<span class="c1"># loop using a Naive bayes classifier</span>
<span class="n">results_df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">()</span>
<span class="k">for</span> <span class="n">model</span> <span class="ow">in</span> <span class="n">model_list</span><span class="p">:</span>
    <span class="n">X_current_model</span> <span class="o">=</span> <span class="n">X</span><span class="p">[</span><span class="n">model</span><span class="p">]</span><span class="o">.</span><span class="n">values</span>

    <span class="n">jupyprint</span><span class="p">(</span><span class="s1">&#39;== Naive Bayes Classifier&#39;</span><span class="p">)</span>
    <span class="n">jupyprint</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;==== Model: survived ~ </span><span class="si">{</span><span class="s2">&quot; + &quot;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">model</span><span class="p">)</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>

    <span class="n">current_model_cross_val_score</span> <span class="o">=</span> <span class="n">cross_val_score</span><span class="p">(</span><span class="n">GaussianNB</span><span class="p">(),</span>
                                                      <span class="n">X_current_model</span><span class="p">,</span>
                                                      <span class="n">y</span><span class="p">,</span>
                                                      <span class="n">cv</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span>
                                                      <span class="n">scoring</span><span class="o">=</span><span class="s1">&#39;accuracy&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>

    <span class="n">jupyprint</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;Accuracy = </span><span class="si">{</span><span class="n">current_model_cross_val_score</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>
    <span class="n">results_df</span><span class="p">[</span><span class="s1">&#39; + &#39;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">model</span><span class="p">)]</span> <span class="o">=</span> <span class="p">[</span><span class="n">current_model_cross_val_score</span><span class="p">]</span>

<span class="n">results_df</span><span class="o">.</span><span class="n">T</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<p>== Naive Bayes Classifier</p>
<p>==== Model: survived ~ fare</p>
<p>Accuracy = 0.7666666666666668</p>
<p>== Naive Bayes Classifier</p>
<p>==== Model: survived ~ fare + age</p>
<p>Accuracy = 0.8466666666666667</p>
<p>== Naive Bayes Classifier</p>
<p>==== Model: survived ~ fare + age + gender_dummy</p>
<p>Accuracy = 0.8</p>
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>0</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>fare</th>
      <td>0.766667</td>
    </tr>
    <tr>
      <th>fare + age</th>
      <td>0.846667</td>
    </tr>
    <tr>
      <th>fare + age + gender_dummy</th>
      <td>0.800000</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
</section>
<section id="other-ways-of-model-building">
<h2>Other ways of model-building<a class="headerlink" href="#other-ways-of-model-building" title="Link to this heading">#</a></h2>
<p>A different method of model building involves the Akaike Information Criterion:</p>
<p><span class="math notranslate nohighlight">\( \text{AIC} = 2k-2\ln({\hat {L}}) \)</span></p>
<p>This metric:</p>
<ul class="simple">
<li><p>Penalizes complexity (more predictors), rewards parsimony - getting good fit with lower number of predictors</p></li>
<li><p>Works with linear regression too! - look at statsmodels output</p></li>
<li><p>Lower (or more negative) AIC is better</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># a function to calculate AIC</span>
<span class="k">def</span><span class="w"> </span><span class="nf">aic</span><span class="p">(</span><span class="n">number_of_parameters</span><span class="p">,</span> <span class="n">log_likelihood</span><span class="p">):</span>
    <span class="k">return</span> <span class="mi">2</span><span class="o">*</span><span class="n">number_of_parameters</span> <span class="o">-</span> <span class="mi">2</span><span class="o">*</span><span class="n">log_likelihood</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># a visualisation of AIC, as a function of model fit (log-likelihood, and complexity)</span>
<span class="n">num_params</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">10</span><span class="p">)</span>
<span class="n">log_likelihood</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">-</span><span class="mi">100</span><span class="p">,</span> <span class="mi">100</span><span class="p">)</span>
<span class="n">num_params</span><span class="p">,</span> <span class="n">log_likelihood</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">meshgrid</span><span class="p">(</span><span class="n">num_params</span><span class="p">,</span> <span class="n">log_likelihood</span><span class="p">)</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">aic</span><span class="p">(</span><span class="n">num_params</span><span class="p">,</span> <span class="n">log_likelihood</span><span class="p">)</span>

<span class="n">fig</span> <span class="o">=</span> <span class="n">go</span><span class="o">.</span><span class="n">Figure</span><span class="p">()</span>
<span class="n">fig</span><span class="o">.</span><span class="n">add_trace</span><span class="p">(</span><span class="n">go</span><span class="o">.</span><span class="n">Surface</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="n">num_params</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="n">log_likelihood</span><span class="p">,</span> <span class="n">z</span><span class="o">=</span><span class="n">y</span><span class="p">))</span><span class="o">.</span><span class="n">update_layout</span><span class="p">(</span><span class="n">scene</span><span class="o">=</span><span class="nb">dict</span><span class="p">(</span>
    <span class="n">xaxis_title</span><span class="o">=</span><span class="s2">&quot;Number of Parameters&quot;</span><span class="p">,</span> <span class="n">yaxis_title</span><span class="o">=</span><span class="s2">&quot;Log-Likelihood&quot;</span><span class="p">,</span> <span class="n">zaxis_title</span><span class="o">=</span><span class="s1">&#39;AIC&#39;</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># show the model with two predictors</span>
<span class="n">fitted_log_reg_mod</span><span class="o">.</span><span class="n">summary</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><table class="simpletable">
<caption>Logit Regression Results</caption>
<tr>
  <th>Dep. Variable:</th>    <td>survived_dummy</td>  <th>  No. Observations:  </th>  <td>    26</td>  
</tr>
<tr>
  <th>Model:</th>                 <td>Logit</td>      <th>  Df Residuals:      </th>  <td>    23</td>  
</tr>
<tr>
  <th>Method:</th>                 <td>MLE</td>       <th>  Df Model:          </th>  <td>     2</td>  
</tr>
<tr>
  <th>Date:</th>            <td>Wed, 28 Jan 2026</td> <th>  Pseudo R-squ.:     </th>  <td>0.5089</td>  
</tr>
<tr>
  <th>Time:</th>                <td>11:43:36</td>     <th>  Log-Likelihood:    </th> <td> -8.2360</td> 
</tr>
<tr>
  <th>converged:</th>             <td>True</td>       <th>  LL-Null:           </th> <td> -16.771</td> 
</tr>
<tr>
  <th>Covariance Type:</th>     <td>nonrobust</td>    <th>  LLR p-value:       </th> <td>0.0001965</td>
</tr>
</table>
<table class="simpletable">
<tr>
      <td></td>         <th>coef</th>     <th>std err</th>      <th>z</th>      <th>P>|z|</th>  <th>[0.025</th>    <th>0.975]</th>  
</tr>
<tr>
  <th>Intercept</th> <td>   -6.7709</td> <td>    2.602</td> <td>   -2.602</td> <td> 0.009</td> <td>  -11.871</td> <td>   -1.671</td>
</tr>
<tr>
  <th>fare</th>      <td>    0.1446</td> <td>    0.065</td> <td>    2.223</td> <td> 0.026</td> <td>    0.017</td> <td>    0.272</td>
</tr>
<tr>
  <th>age</th>       <td>    0.1101</td> <td>    0.055</td> <td>    2.009</td> <td> 0.045</td> <td>    0.003</td> <td>    0.218</td>
</tr>
</table></div></div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># the AIC from the model with two predictors</span>
<span class="n">fitted_log_reg_mod</span><span class="o">.</span><span class="n">aic</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>np.float64(22.47203433679345)
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># show the model parameters</span>
<span class="n">fitted_log_reg_mod</span><span class="o">.</span><span class="n">params</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Intercept   -6.770868
fare         0.144592
age          0.110099
dtype: float64
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># show the model log-likelihood</span>
<span class="n">fitted_log_reg_mod</span><span class="o">.</span><span class="n">llf</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>np.float64(-8.236017168396724)
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># recalculate the AIC, manually</span>
<span class="n">aic</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">fitted_log_reg_mod</span><span class="o">.</span><span class="n">params</span><span class="p">),</span> <span class="n">fitted_log_reg_mod</span><span class="o">.</span><span class="n">llf</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>np.float64(22.47203433679345)
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># show the single predictor model</span>
<span class="n">fitted_log_reg_mod_single_predictor</span><span class="o">.</span><span class="n">summary</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><table class="simpletable">
<caption>Logit Regression Results</caption>
<tr>
  <th>Dep. Variable:</th>    <td>survived_dummy</td>  <th>  No. Observations:  </th>  <td>    26</td> 
</tr>
<tr>
  <th>Model:</th>                 <td>Logit</td>      <th>  Df Residuals:      </th>  <td>    24</td> 
</tr>
<tr>
  <th>Method:</th>                 <td>MLE</td>       <th>  Df Model:          </th>  <td>     1</td> 
</tr>
<tr>
  <th>Date:</th>            <td>Wed, 28 Jan 2026</td> <th>  Pseudo R-squ.:     </th>  <td>0.3104</td> 
</tr>
<tr>
  <th>Time:</th>                <td>11:43:36</td>     <th>  Log-Likelihood:    </th> <td> -11.565</td>
</tr>
<tr>
  <th>converged:</th>             <td>True</td>       <th>  LL-Null:           </th> <td> -16.771</td>
</tr>
<tr>
  <th>Covariance Type:</th>     <td>nonrobust</td>    <th>  LLR p-value:       </th> <td>0.001252</td>
</tr>
</table>
<table class="simpletable">
<tr>
      <td></td>         <th>coef</th>     <th>std err</th>      <th>z</th>      <th>P>|z|</th>  <th>[0.025</th>    <th>0.975]</th>  
</tr>
<tr>
  <th>Intercept</th> <td>   -2.5415</td> <td>    0.944</td> <td>   -2.691</td> <td> 0.007</td> <td>   -4.392</td> <td>   -0.691</td>
</tr>
<tr>
  <th>fare</th>      <td>    0.0828</td> <td>    0.039</td> <td>    2.146</td> <td> 0.032</td> <td>    0.007</td> <td>    0.158</td>
</tr>
</table></div></div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># show the AIC of the single predictor model (compare this to that of the two predictor model - lower is better!)</span>
<span class="n">fitted_log_reg_mod_single_predictor</span><span class="o">.</span><span class="n">aic</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>np.float64(27.13041810470826)
</pre></div>
</div>
</div>
</div>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "lisds/dsip",
            ref: "main",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./."
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

                </article>
              

              
              
              
              
                <footer class="prev-next-footer d-print-none">
                  
<div class="prev-next-area">
    <a class="left-prev"
       href="multiple_predictors_statistical_adjustment.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title">Statistical Adjustment in Multi-predictor Linear Regression Models</p>
      </div>
    </a>
    <a class="right-next"
       href="cost_functions.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">Fitting models with different cost functions</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div>
                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">


  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#more-logistic-regression">More Logistic Regression</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#our-first-logistic-model-and-two-perspectives-on-the-cost-function">Our first logistic model - and two perspectives on the cost function</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#logistic-regression-in-3d-i-e-with-two-predictors">Logistic regression in 3D (i.e. with two predictors)</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#model-evaluation-goodness-of-fit-and-model-comparison">Model Evaluation (Goodness-of-Fit) and Model Comparison</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#accuracy-for-models-with-a-categorical-outcome-variable">Accuracy (for models with a categorical outcome variable)</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#test-train-splits">Test/Train splits</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#cross-validation">Cross-validation</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#other-ways-of-model-building">Other ways of model-building</a></li>
</ul>
  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By Matthew Brett and Peter Rush
</p>

  </div>
  
  <div class="footer-item">
    

  <p class="copyright">
    
      © Copyright 2023.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b"></script>
<script src="_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>